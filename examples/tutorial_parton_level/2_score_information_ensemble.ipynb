{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner tutorial 2: SALLY, Fisher information, and ensemble methods (parton level)\n",
    "\n",
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first tutorial you saw how to set up a process with MadMiner, generate events and turn them into training samples, and construct likelihood ratio estimators from them. Here we introduce some alternative steps which will lead us to an estimator for the score at a reference point (SALLY) and the expected Fisher information. Along the way, we'll introduce some powerful ensemble methods.\n",
    "\n",
    "If you're not familiar with SALLY, please have a look at [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013)\n",
    "or, for more details, [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). For the Fisher information part, see the original publication [\"Better Higgs Measurements Through Information Geometry\"](https://arxiv.org/abs/1612.05261) or a more detailed, pedagogical introduction in Chapter 4 of [\"New Ideas for Effective Higgs Measurements\"](https://inspirehep.net/record/1624219)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've run the first tutorial before executing this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from madminer.sampling import SampleAugmenter, constant_benchmark_theta\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "from madminer.fisherinformation import FisherInformation\n",
    "from madminer.plotting import plot_fisher_information_contours_2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MadMiner uses the Python `logging` module to provide additional information and debugging output. You can choose how much of this output you want to see by switching the level in the following lines to `logging.DEBUG` or `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.DEBUG\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 5.: see tutorial 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have run steps 1 through 5 of the first tutorial, and thus have a MadMiner file with observables and event weights ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the next step is handled by the MadMiner class `SampleAugmenter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:47 madminer.sampling    INFO    Loading data from data/madminer_example_shuffled.h5\n",
      "11:47 madminer.sampling    INFO    Found 2 parameters\n",
      "11:47 madminer.sampling    DEBUG      CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-10.0, 10.0))\n",
      "11:47 madminer.sampling    DEBUG      CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-10.0, 1.0))\n",
      "11:47 madminer.sampling    INFO    Did not find nuisance parameters\n",
      "11:47 madminer.sampling    INFO    Found 6 benchmarks, of which 6 physical\n",
      "11:47 madminer.sampling    DEBUG      sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00\n",
      "11:47 madminer.sampling    DEBUG      w: CWL2 = 10.00, CPWL2 = 0.00e+00\n",
      "11:47 madminer.sampling    DEBUG      morphing_basis_vector_2: CWL2 = -2.43e+00, CPWL2 = -9.92e+00\n",
      "11:47 madminer.sampling    DEBUG      morphing_basis_vector_3: CWL2 = -8.02e+00, CPWL2 = -5.62e+00\n",
      "11:47 madminer.sampling    DEBUG      morphing_basis_vector_4: CWL2 = -7.13e+00, CPWL2 = -1.36e+00\n",
      "11:47 madminer.sampling    DEBUG      morphing_basis_vector_5: CWL2 = 8.52, CPWL2 = -3.17e+00\n",
      "11:47 madminer.sampling    INFO    Found 2 observables\n",
      "11:47 madminer.sampling    DEBUG      0 pt_j1\n",
      "11:47 madminer.sampling    DEBUG      1 delta_phi_jj\n",
      "11:47 madminer.sampling    INFO    Found 6537 events\n",
      "11:47 madminer.sampling    INFO    Found morphing setup with 6 components\n"
     ]
    }
   ],
   "source": [
    "sa = SampleAugmenter('data/madminer_example_shuffled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant `SampleAugmenter` function for local score estimators is `extract_samples_train_local()`. As before, for the argument `theta` you can use the helper functions `constant_benchmark_theta()`, `multiple_benchmark_thetas()`, `constant_morphing_theta()`, `multiple_morphing_thetas()`, and `random_morphing_thetas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:47 madminer.sampling    INFO    Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "11:47 madminer.sampling    DEBUG   Starting sample extraction\n",
      "11:47 madminer.sampling    DEBUG   Augmented data requested:\n",
      "11:47 madminer.sampling    DEBUG     (u'score', 0)\n",
      "11:47 madminer.sampling    DEBUG   Benchmark cross sections [pb]: [3.73110584e-05 2.41485478e-04 5.57774798e-04 4.94442997e-04\n",
      " 2.84443497e-04 2.06884202e-04]\n",
      "11:47 madminer.sampling    DEBUG   Drawing 100000 events for the following thetas:\n",
      "11:47 madminer.sampling    DEBUG     theta 0 = [0. 0.] (sampling)\n",
      "11:47 madminer.sampling    DEBUG     Cumulative probability (should be close to 1): 0.9999999999999912\n",
      "11:47 madminer.sampling    INFO    Effective number of samples: 3269.0000000001205\n"
     ]
    }
   ],
   "source": [
    "x, theta, t_xz = sa.extract_samples_train_local(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. Train one network to estimate score and Fisher information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build a neural network. Only this time, instead of the likelihood ratio itself, we will estimate the gradient of the log likelihood with respect to the theory parameters -- the score. To be precise, the output of the neural network is an estimate of the score at some reference parameter point, for instance the Standard Model. A neural network that estimates this \"local\" score can be used to calculate the Fisher information at that point. The estimated score can also be used as a machine learning version of Optimal Observables, and likelihoods can be estimated based on density estimation in the estimated score space. This method for likelihood ratio estimation is called SALLY, and there is a closely related version called SALLINO. Both are explained in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020).\n",
    "\n",
    "Again, the central object for this is the `madminer.ml.MLForge` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:47 madminer.ml          INFO    Starting training\n",
      "11:47 madminer.ml          INFO      Method:                 sally\n",
      "11:47 madminer.ml          INFO      Training data:          x at data/samples/x_train.npy\n",
      "11:47 madminer.ml          INFO                              t_xz (theta0) at data/samples/t_xz_train.npy\n",
      "11:47 madminer.ml          INFO      Features:               all\n",
      "11:47 madminer.ml          INFO      Method:                 sally\n",
      "11:47 madminer.ml          INFO      Hidden layers:          (100, 100, 100)\n",
      "11:47 madminer.ml          INFO      Activation function:    tanh\n",
      "11:47 madminer.ml          INFO      Batch size:             200\n",
      "11:47 madminer.ml          INFO      Optimizer:              amsgrad\n",
      "11:47 madminer.ml          INFO      Epochs:                 50\n",
      "11:47 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:47 madminer.ml          INFO      Validation split:       0.25\n",
      "11:47 madminer.ml          INFO      Early stopping:         True\n",
      "11:47 madminer.ml          INFO      Scale inputs:           True\n",
      "11:47 madminer.ml          INFO      Shuffle labels          False\n",
      "11:47 madminer.ml          INFO      Samples:                all\n",
      "11:47 madminer.ml          INFO    Loading training data\n",
      "11:47 madminer.ml          INFO    Found 100000 samples with 2 parameters and 2 observables\n",
      "11:47 madminer.ml          INFO    Rescaling inputs\n",
      "11:47 madminer.ml          DEBUG   Observable ranges:\n",
      "11:47 madminer.ml          DEBUG     x_1: mean -1.0863053567788938e-13, std 0.9999999999999963, range -1.195981150174883 ... 7.707667596513565\n",
      "11:47 madminer.ml          DEBUG     x_2: mean -9.020340030474472e-17, std 1.0000000000000073, range -1.5319918567479223 ... 1.5593267872302359\n",
      "11:47 madminer.ml          INFO    Creating model for method sally\n",
      "11:47 madminer.ml          INFO    Training model\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training on CPU with single precision\n",
      "11:47 madminer.utils.ml.tr DEBUG   Initialising training data\n",
      "11:47 madminer.utils.ml.tr DEBUG   Setting up optimizer\n",
      "11:47 madminer.utils.ml.tr DEBUG   Using early stopping with infinite patience\n",
      "11:47 madminer.utils.ml.tr DEBUG   Will print training progress every 1 epochs\n",
      "11:47 madminer.utils.ml.tr DEBUG   Beginning main training loop\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 1 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.001\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   1: train loss  0.1357 (mse_score: 0.136)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss   0.1074 (mse_score: 0.107)\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 2 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.00095409547635\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   2: train loss  0.1057 (mse_score: 0.106)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss   0.1008 (mse_score: 0.101)\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 3 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.000910298177992\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   3: train loss  0.1015 (mse_score: 0.101)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss   0.0977 (mse_score: 0.098)\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 4 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.000868511373751\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   4: train loss  0.0984 (mse_score: 0.098)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss   0.0958 (mse_score: 0.096)\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 5 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.000828642772855\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   5: train loss  0.0967 (mse_score: 0.097)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss   0.0939 (mse_score: 0.094)\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 6 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.000790604321091\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   6: train loss  0.0946 (mse_score: 0.095)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss   0.0931 (mse_score: 0.093)\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 7 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.000754312006335\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   7: train loss  0.0935 (mse_score: 0.093)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss   0.0953 (mse_score: 0.095)\n",
      "11:47 madminer.utils.ml.tr DEBUG   Training epoch 8 / 50\n",
      "11:47 madminer.utils.ml.tr DEBUG   Learning rate: 0.000719685673001\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch   8: train loss  0.0919 (mse_score: 0.092)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0920 (mse_score: 0.092)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 9 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000686648845004\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch   9: train loss  0.0904 (mse_score: 0.090)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0912 (mse_score: 0.091)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 10 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.00065512855686\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  10: train loss  0.0894 (mse_score: 0.089)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0909 (mse_score: 0.091)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 11 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000625055192527\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  11: train loss  0.0886 (mse_score: 0.089)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0908 (mse_score: 0.091)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 12 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000596362331659\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  12: train loss  0.0870 (mse_score: 0.087)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0877 (mse_score: 0.088)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 13 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000568986602902\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  13: train loss  0.0865 (mse_score: 0.086)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0889 (mse_score: 0.089)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 14 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000542867543932\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  14: train loss  0.0858 (mse_score: 0.086)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0881 (mse_score: 0.088)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 15 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000517947467923\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  15: train loss  0.0849 (mse_score: 0.085)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0865 (mse_score: 0.086)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 16 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000494171336132\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  16: train loss  0.0841 (mse_score: 0.084)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0859 (mse_score: 0.086)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 17 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000471486636346\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  17: train loss  0.0832 (mse_score: 0.083)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0850 (mse_score: 0.085)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 18 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000449843266897\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  18: train loss  0.0828 (mse_score: 0.083)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0850 (mse_score: 0.085)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 19 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000429193426013\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  19: train loss  0.0821 (mse_score: 0.082)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0851 (mse_score: 0.085)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 20 / 50\n",
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000409491506238\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch  20: train loss  0.0818 (mse_score: 0.082)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss   0.0838 (mse_score: 0.084)\n",
      "11:48 madminer.utils.ml.tr DEBUG   Training epoch 21 / 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:48 madminer.utils.ml.tr DEBUG   Learning rate: 0.000390693993705\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  21: train loss  0.0811 (mse_score: 0.081)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0845 (mse_score: 0.084)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 22 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000372759372031\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  22: train loss  0.0807 (mse_score: 0.081)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0829 (mse_score: 0.083)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 23 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000355648030622\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  23: train loss  0.0803 (mse_score: 0.080)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0836 (mse_score: 0.084)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 24 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.00033932217719\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  24: train loss  0.0798 (mse_score: 0.080)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0831 (mse_score: 0.083)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 25 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000323745754282\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  25: train loss  0.0795 (mse_score: 0.079)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0816 (mse_score: 0.082)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 26 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000308884359648\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  26: train loss  0.0792 (mse_score: 0.079)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0828 (mse_score: 0.083)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 27 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000294705170255\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  27: train loss  0.0791 (mse_score: 0.079)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0813 (mse_score: 0.081)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 28 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000281176869797\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  28: train loss  0.0786 (mse_score: 0.079)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0806 (mse_score: 0.081)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 29 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000268269579528\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  29: train loss  0.0783 (mse_score: 0.078)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0809 (mse_score: 0.081)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 30 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.00025595479227\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  30: train loss  0.0780 (mse_score: 0.078)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0806 (mse_score: 0.081)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 31 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000244205309455\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  31: train loss  0.0778 (mse_score: 0.078)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0803 (mse_score: 0.080)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 32 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000232995181052\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  32: train loss  0.0775 (mse_score: 0.078)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss   0.0801 (mse_score: 0.080)\n",
      "11:49 madminer.utils.ml.tr DEBUG   Training epoch 33 / 50\n",
      "11:49 madminer.utils.ml.tr DEBUG   Learning rate: 0.000222299648253\n",
      "11:50 madminer.utils.ml.tr INFO    Epoch  33: train loss  0.0775 (mse_score: 0.077)\n",
      "11:50 madminer.utils.ml.tr INFO               val. loss   0.0801 (mse_score: 0.080)\n",
      "11:50 madminer.utils.ml.tr DEBUG   Training epoch 34 / 50\n",
      "11:50 madminer.utils.ml.tr DEBUG   Learning rate: 0.000212095088792\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename='data/samples/x_train.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train.npy',\n",
    "    verbose=\"all\",\n",
    ")\n",
    "\n",
    "forge.save('models/sally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the SM score on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.load('models/sally')\n",
    "\n",
    "t_hat = forge.evaluate(\n",
    "    x='data/samples/x_test.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the estimated score and how it is related to the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/samples/x_test.npy')\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "\n",
    "    sc = plt.scatter(x[::10,0], x[::10,1], c=t_hat[::10,i], s=10., cmap='viridis', vmin=-0.8, vmax=0.4)\n",
    "    cbar = plt.colorbar(sc)\n",
    "\n",
    "    cbar.set_label(r'$\\hat{t}_' + str(i) + r'(x | \\theta_{ref})$')\n",
    "    plt.xlabel(r'$p_{T,j1}$ [GeV]')\n",
    "    plt.ylabel(r'$\\Delta \\phi_{jj}$')\n",
    "    plt.xlim(10.,400.)\n",
    "    plt.ylim(-3.15,3.15)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can use SALLY estimators to estimate the expected Fisher information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_information, _ = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally',\n",
    "    unweighted_x_sample_file='data/samples/x_test.npy',\n",
    "    luminosity=3000000.\n",
    ")\n",
    "\n",
    "print('Kinematic Fisher information after 3000 ifb:\\n{}'.format(fisher_information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Fisher information with contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information],\n",
    "    xrange=(-1,1),\n",
    "    yrange=(-1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a single neural network to estimate the likelihood ratio, score, or Fisher information, we can use an ensemble of such estimators. That provides us with a more reliable mean prediction as well as a measure of the uncertainty. The class `madminer.ml.EnsembleForge` automates this process. Currently, it only supports SALLY estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleForge(estimators=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EnsembleForge` object has very similar functions as `MLForge`. In particular, we can train all estimators simultaneously with `train_all()` and save the ensemble to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x_filename='data/samples/x_train.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train.npy',\n",
    "    n_epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "ensemble.save('models/sally_ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the ensemble similarly to the individual networks. Let's stick to the estimation of the Fisher information. There are two different ways to take the ensemble average:\n",
    "\n",
    "- `mode='information'`: We can calculate the Fisher information for each estimator in the ensemble, and then take the mean and the covariance over the ensemble. This has the advantage that it provides a direct measure of the uncertainty of the prediction.\n",
    "- `mode='score'`: We can calculate the score for each event and estimator, take the ensemble mean for the score of each event, and then calculate the Fisher information based on the mean scores. This is expected to be more precise (since the score estimates will be more precise, and the nonlinearity in the Fisher info calculation amplifies any error in the score estimation). But calculating the covariance in this approach is computationally not feasible, so there will be no error bands.\n",
    "\n",
    "By default, MadMiner uses the 'score' mode. Here we will use the 'information' mode just to show the nice uncertainty bands we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5')\n",
    "\n",
    "fisher_information_mean, fisher_information_covariance = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally_ensemble',\n",
    "    luminosity=3000000.,\n",
    "    mode='information'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance can be propagated to the Fisher distance contour plot easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information_mean],\n",
    "    [fisher_information_covariance],\n",
    "    xrange=(-1,1),\n",
    "    yrange=(-1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of the second part of this tutorial. If you have questions, please have a look at the papers, the module documentation, or drop us an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
