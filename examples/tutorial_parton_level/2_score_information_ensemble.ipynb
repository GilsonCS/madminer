{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner tutorial 2: SALLY, Fisher information, and ensemble methods (parton level)\n",
    "\n",
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first tutorial you saw how to set up a process with MadMiner, generate events and turn them into training samples, and construct likelihood ratio estimators from them. Here we introduce some alternative steps which will lead us to an estimator for the score at a reference point (SALLY) and the expected Fisher information. Along the way, we'll introduce some powerful ensemble methods.\n",
    "\n",
    "If you're not familiar with SALLY, please have a look at [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013)\n",
    "or, for more details, [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). For the Fisher information part, see the original publication [\"Better Higgs Measurements Through Information Geometry\"](https://arxiv.org/abs/1612.05261) or a more detailed, pedagogical introduction in Chapter 4 of [\"New Ideas for Effective Higgs Measurements\"](https://inspirehep.net/record/1624219)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've run the first tutorial before executing this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from madminer import sampling\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.ml import ScoreEstimator, Ensemble\n",
    "from madminer.fisherinformation import FisherInformation\n",
    "from madminer.plotting import plot_fisher_information_contours_2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MadMiner uses the Python `logging` module to provide additional information and debugging output. You can choose how much of this output you want to see by switching the level in the following lines to `logging.DEBUG` or `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 5.: see tutorial 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have run steps 1 through 5 of the first tutorial, and thus have a MadMiner file with observables and event weights ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the next step is handled by the MadMiner class `SampleAugmenter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:59 madminer.analysis    INFO    Loading data from data/madminer_example_shuffled.h5\n",
      "14:59 madminer.analysis    INFO    Found 2 parameters\n",
      "14:59 madminer.analysis    INFO    Did not find nuisance parameters\n",
      "14:59 madminer.analysis    INFO    Found 6 benchmarks, of which 6 physical\n",
      "14:59 madminer.analysis    INFO    Found 2 observables\n",
      "14:59 madminer.analysis    INFO    Found 6537 events\n",
      "14:59 madminer.analysis    INFO    Found morphing setup with 6 components\n"
     ]
    }
   ],
   "source": [
    "sampler = SampleAugmenter('data/madminer_example_shuffled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant `SampleAugmenter` function for local score estimators is `extract_samples_train_local()`. As before, for the argument `theta` you can use the helper functions `sampling.benchmark()`, `sampling.benchmarks()`, `sampling.morphing_point()`, `sampling.morphing_points()`, and `sampling.random_morphing_points()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:59 madminer.sampling    INFO    Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "14:59 madminer.sampling    INFO    Effective number of samples: 5230.0\n"
     ]
    }
   ],
   "source": [
    "x, theta, t_xz, _ = sampler.sample_train_local(\n",
    "    theta=sampling.benchmark('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. Train one network to estimate score and Fisher information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build a neural network. Only this time, instead of the likelihood ratio itself, we will estimate the gradient of the log likelihood with respect to the theory parameters -- the score. To be precise, the output of the neural network is an estimate of the score at some reference parameter point, for instance the Standard Model. A neural network that estimates this \"local\" score can be used to calculate the Fisher information at that point. The estimated score can also be used as a machine learning version of Optimal Observables, and likelihoods can be estimated based on density estimation in the estimated score space. This method for likelihood ratio estimation is called SALLY, and there is a closely related version called SALLINO. Both are explained in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020).\n",
    "\n",
    "The central object for this is the `madminer.ml.ScoreEstimator` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = ScoreEstimator(n_hidden=(20,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14:59 madminer.ml          INFO    Starting training\n",
      "14:59 madminer.ml          INFO      Batch size:             200\n",
      "14:59 madminer.ml          INFO      Optimizer:              amsgrad\n",
      "14:59 madminer.ml          INFO      Epochs:                 50\n",
      "14:59 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "14:59 madminer.ml          INFO      Validation split:       0.25\n",
      "14:59 madminer.ml          INFO      Early stopping:         True\n",
      "14:59 madminer.ml          INFO      Scale inputs:           True\n",
      "14:59 madminer.ml          INFO      Shuffle labels          False\n",
      "14:59 madminer.ml          INFO      Samples:                all\n",
      "14:59 madminer.ml          INFO    Loading training data\n",
      "14:59 madminer.ml          INFO    Found 100000 samples with 2 parameters and 2 observables\n",
      "14:59 madminer.ml          INFO    Rescaling inputs\n",
      "14:59 madminer.ml          INFO    Creating model\n",
      "14:59 madminer.ml          INFO    Training model\n",
      "14:59 madminer.utils.ml.tr INFO    Epoch   3: train loss  0.14074 (mse_score:  0.141)\n",
      "14:59 madminer.utils.ml.tr INFO               val. loss   0.14146 (mse_score:  0.141)\n",
      "14:59 madminer.utils.ml.tr INFO    Epoch   6: train loss  0.11731 (mse_score:  0.117)\n",
      "14:59 madminer.utils.ml.tr INFO               val. loss   0.12338 (mse_score:  0.123)\n",
      "14:59 madminer.utils.ml.tr INFO    Epoch   9: train loss  0.11180 (mse_score:  0.112)\n",
      "14:59 madminer.utils.ml.tr INFO               val. loss   0.11818 (mse_score:  0.118)\n",
      "14:59 madminer.utils.ml.tr INFO    Epoch  12: train loss  0.10865 (mse_score:  0.109)\n",
      "14:59 madminer.utils.ml.tr INFO               val. loss   0.11453 (mse_score:  0.115)\n",
      "15:00 madminer.utils.ml.tr INFO    Epoch  15: train loss  0.10630 (mse_score:  0.106)\n",
      "15:00 madminer.utils.ml.tr INFO               val. loss   0.11202 (mse_score:  0.112)\n",
      "15:00 madminer.utils.ml.tr INFO    Epoch  18: train loss  0.10462 (mse_score:  0.105)\n",
      "15:00 madminer.utils.ml.tr INFO               val. loss   0.11001 (mse_score:  0.110)\n",
      "15:00 madminer.utils.ml.tr INFO    Epoch  21: train loss  0.10337 (mse_score:  0.103)\n",
      "15:00 madminer.utils.ml.tr INFO               val. loss   0.10884 (mse_score:  0.109)\n",
      "15:00 madminer.utils.ml.tr INFO    Epoch  24: train loss  0.10249 (mse_score:  0.102)\n",
      "15:00 madminer.utils.ml.tr INFO               val. loss   0.10784 (mse_score:  0.108)\n",
      "15:00 madminer.utils.ml.tr INFO    Epoch  27: train loss  0.10183 (mse_score:  0.102)\n",
      "15:00 madminer.utils.ml.tr INFO               val. loss   0.10733 (mse_score:  0.107)\n",
      "15:00 madminer.utils.ml.tr INFO    Epoch  30: train loss  0.10134 (mse_score:  0.101)\n",
      "15:00 madminer.utils.ml.tr INFO               val. loss   0.10667 (mse_score:  0.107)\n",
      "15:01 madminer.utils.ml.tr INFO    Epoch  33: train loss  0.10096 (mse_score:  0.101)\n",
      "15:01 madminer.utils.ml.tr INFO               val. loss   0.10628 (mse_score:  0.106)\n",
      "15:01 madminer.utils.ml.tr INFO    Epoch  36: train loss  0.10066 (mse_score:  0.101)\n",
      "15:01 madminer.utils.ml.tr INFO               val. loss   0.10597 (mse_score:  0.106)\n",
      "15:01 madminer.utils.ml.tr INFO    Epoch  39: train loss  0.10041 (mse_score:  0.100)\n",
      "15:01 madminer.utils.ml.tr INFO               val. loss   0.10572 (mse_score:  0.106)\n",
      "15:01 madminer.utils.ml.tr INFO    Epoch  42: train loss  0.10021 (mse_score:  0.100)\n",
      "15:01 madminer.utils.ml.tr INFO               val. loss   0.10548 (mse_score:  0.105)\n",
      "15:02 madminer.utils.ml.tr INFO    Epoch  45: train loss  0.10004 (mse_score:  0.100)\n",
      "15:02 madminer.utils.ml.tr INFO               val. loss   0.10525 (mse_score:  0.105)\n",
      "15:02 madminer.utils.ml.tr INFO    Epoch  48: train loss  0.09990 (mse_score:  0.100)\n",
      "15:02 madminer.utils.ml.tr INFO               val. loss   0.10511 (mse_score:  0.105)\n",
      "15:02 madminer.utils.ml.tr INFO    Early stopping did not improve performance\n"
     ]
    }
   ],
   "source": [
    "estimator.train(\n",
    "    method='sally',\n",
    "    x='data/samples/x_train.npy',\n",
    "    t_xz='data/samples/t_xz_train.npy',\n",
    ")\n",
    "\n",
    "estimator.save('models/sally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the SM score on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'FileNotFoundError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b53f819327e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/sally'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m t_hat = estimator.evaluate_score(\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/samples/x_test.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/Users/johannbrehmer/work/projects/madminer/madminer/madminer/ml.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m   1435\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnuisance_project_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             )\n\u001b[0;32m-> 1437\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Did not find nuisance profiling / projection setup in %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnuisance_profile_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'FileNotFoundError' is not defined"
     ]
    }
   ],
   "source": [
    "estimator.load('models/sally')\n",
    "\n",
    "t_hat = estimator.evaluate_score(\n",
    "    x='data/samples/x_test.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the estimated score and how it is related to the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/samples/x_test.npy')\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "\n",
    "    sc = plt.scatter(x[::10,0], x[::10,1], c=t_hat[::10,i], s=10., cmap='viridis', vmin=-0.8, vmax=0.4)\n",
    "    cbar = plt.colorbar(sc)\n",
    "\n",
    "    cbar.set_label(r'$\\hat{t}_' + str(i) + r'(x | \\theta_{ref})$')\n",
    "    plt.xlabel(r'$p_{T,j1}$ [GeV]')\n",
    "    plt.ylabel(r'$\\Delta \\phi_{jj}$')\n",
    "    plt.xlim(10.,400.)\n",
    "    plt.ylim(-3.15,3.15)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can use SALLY estimators to estimate the expected Fisher information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_information, _ = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally',\n",
    "    luminosity=3000000.\n",
    ")\n",
    "\n",
    "print('Kinematic Fisher information after 3000 ifb:\\n{}'.format(fisher_information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Fisher information with contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information],\n",
    "    xrange=(-1,1),\n",
    "    yrange=(-1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a single neural network to estimate the likelihood ratio, score, or Fisher information, we can use an ensemble of such estimators. That provides us with a more reliable mean prediction as well as a measure of the uncertainty. The class `madminer.ml.EnsembleForge` automates this process. Currently, it only supports SALLY estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [ScoreEstimator(n_hidden=(20,)) for _ in range(5)]\n",
    "\n",
    "ensemble = Ensemble(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EnsembleForge` object has very similar functions as `MLForge`. In particular, we can train all estimators simultaneously with `train_all()` and save the ensemble to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x='data/samples/x_train.npy',\n",
    "    t_xz='data/samples/t_xz_train.npy',\n",
    "    n_epochs=5,\n",
    ")\n",
    "\n",
    "ensemble.save('models/sally_ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the ensemble similarly to the individual networks. Let's stick to the estimation of the Fisher information. There are two different ways to take the ensemble average:\n",
    "\n",
    "- `mode='information'`: We can calculate the Fisher information for each estimator in the ensemble, and then take the mean and the covariance over the ensemble. This has the advantage that it provides a direct measure of the uncertainty of the prediction.\n",
    "- `mode='score'`: We can calculate the score for each event and estimator, take the ensemble mean for the score of each event, and then calculate the Fisher information based on the mean scores. This is expected to be more precise (since the score estimates will be more precise, and the nonlinearity in the Fisher info calculation amplifies any error in the score estimation). But calculating the covariance in this approach is computationally not feasible, so there will be no error bands.\n",
    "\n",
    "By default, MadMiner uses the 'score' mode. Here we will use the 'information' mode just to show the nice uncertainty bands we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5')\n",
    "\n",
    "fisher_information_mean, fisher_information_covariance = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally_ensemble',\n",
    "    luminosity=3000000.,\n",
    "    mode='information'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance can be propagated to the Fisher distance contour plot easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information_mean],\n",
    "    [fisher_information_covariance],\n",
    "    xrange=(-1,1),\n",
    "    yrange=(-1,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of the second part of this tutorial. If you have questions, please have a look at the papers, the module documentation, or drop us an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
