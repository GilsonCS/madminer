{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner tutorial 1: From cards to likelihood ratios (parton level)\n",
    "\n",
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll introduce the basic MadMiner workflow, using parton-level samples with simple smearing functions. We'll show you how to use MadMiner to generate events, extract training data, and train neural networks to estimate likelihood ratios.\n",
    "\n",
    "This tutorial does not try to explain the inference methods. To understand what MadMiner is doing, please have a look at some papers first. In\n",
    "[\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013)\n",
    "we explain the basic idea of most of the methods presented here, while [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020) is an extensive 65-page handbook going through the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you execute this notebook, make sure you have running installations of MadGraph, Pythia, and Delphes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from madminer.core import MadMiner\n",
    "from madminer.lhe import LHEProcessor\n",
    "from madminer.sampling import combine_and_shuffle\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import constant_benchmark_theta, multiple_benchmark_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge\n",
    "from madminer.plotting import plot_2d_morphing_basis, plot_distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter here the path to your MG5 root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MadMiner uses the Python `logging` module to provide additional information and debugging output. You can choose how much of this output you want to see by switching the level in the following lines to `logging.DEBUG` or `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example process, we'll simulate VBF Higgs production with a decay into two photons. We'll try to measure two EFT coefficients that affect this process.\n",
    "\n",
    "Have a look at the `cards` folder. You'll find text files (\"cards\") that specify the process simulation in typical MadGraph convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a `MadMiner` instance, the first important step is the definition of the parameter space. Each model parameter is characterized by a name as well as the LHA block and ID.\n",
    "\n",
    "If morphing is used, one also has to specify the maximal power with which the parameter contributes to the squared matrix element. For instance, a parameter that contributes only to one vertex, will typically have `morphing_max_power=2`, while a parameter that contributes to two vertices usually has `morphing_max_power=4`. Exceptions arise for instance when the interference effects between the SM and dimension-six operators are modelled, but the square of the dimension-six amplitude (subleading in 1/Lambda) is not taken into account, in which case `morphing_max_power=1`. The `parameter_range` argument defines the range of parameter values that are used for the automatic optimization of the morphing basis.\n",
    "\n",
    "Finally, the parameter values theta used internally by MadMiner and the parameter values written to the param_card (or reweight_card) given to MadGraph do not have to be exactly the same. With the option `parm_card_transform`, the user can supply a one-parameter function that maps a parameter value theta to the value given to MadGraph. This string is a python expression, in which `theta` is parsed as the parameter value. For instance, if the internal parameters are in the range (0, 1), but should be linearly scaled to (0, 100) in the param_card, one would have to use `param_card_transform=\"100*theta\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = MadMiner()\n",
    "\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=2,\n",
    "    parameter_name='CWL2',\n",
    "    morphing_max_power=2,\n",
    "    param_card_transform=\"16.52*theta\",\n",
    "    parameter_range=(-10.,10.)\n",
    ")\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=5,\n",
    "    parameter_name='CPWL2',\n",
    "    morphing_max_power=2,\n",
    "    param_card_transform=\"16.52*theta\",\n",
    "    parameter_range=(-10.,1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define benchmark points (evaluation points for |M|^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of all the points at which the weights (squared matrix elements) should be evaluated by MadGraph. We call these points \"benchmarks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set benchmarks by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define benchmarks by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.add_benchmark(\n",
    "    {'CWL2':0., 'CPWL2':0.},\n",
    "    'sm'\n",
    ")\n",
    "miner.add_benchmark(\n",
    "    {'CWL2':10., 'CPWL2':0.},\n",
    "    'w'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphing setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If morphing is used, the function `set_morphing` has to be called. With the option `include_existing_benchmarks=True`, MadMiner will keep all the benchmark points defined beforehand and run a simple optimization algorithm to fix the remaining ones for the basis (which may be none). Otherwise, MadMiner will optimize the full basis and forget about all previously defined benchmark points.\n",
    "\n",
    "The other important keyword is `max_overall_power`. This sets the maximal combined power with which all parameters together contribute to the squared matrix element (cross section). This constraint is in addition to the `morphing_max_power` keyword of `add_parameter()` (see above). For instance, if there are two parameters of interest `a` and `b`, and it is known that there are contributions proportional to `1` (constant), `a`, `a^2`, `b`, `b^2`, and `ab`, then `max_overall_power=2`. If there are also contributions proportional to `a^2 b`, `a b^2`, and `a^2 b^2`, then `max_overall_power=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "miner.set_morphing(\n",
    "    include_existing_benchmarks=True,\n",
    "    max_overall_power=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the resulting morphing basis and the \"morphing error\", i.e. the sum of squared morphing weights as a function of the parameter space. The black dots mark the benchmarks at which the squared matrix element will be evaluated when MadGraph is run. In between them, the morphing will interpolate. Since the morphing knows about the functional form of the amplitudes, there is only a small numerical uncertainty in this interpolation, we expect that the color in this plot is indicative of this uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_2d_morphing_basis(\n",
    "    miner.morpher,\n",
    "    xlabel=r'$c_{W} v^2 / \\Lambda^2$',\n",
    "    ylabel=r'$c_{\\tilde{W}} v^2 / \\Lambda^2$',\n",
    "    xrange=(-10.,10.),\n",
    "    yrange=(-10.,10.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space, benchmark points, and morphing setup are saved in a HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.save('data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can now be loaded again with `miner.load(filename)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save settings and run MadGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, MadMiner starts MadGraph to generate events and calculate the weights. You have to provide paths to the process card, run card, param card (the entries corresponding to the parameters of interest will be automatically adapted), and an empty reweight card. Log files in the `log_directory` folder collect the MadGraph output and are important for debugging.\n",
    "\n",
    "The `sample_benchmark` option can be used to specify which benchmark should be used for sampling. If it is not used, MadMiner will automatically use the benchmark that was added first.\n",
    "\n",
    "Finally, if your default Python interpreter is 3.x, you will want to use `python2_override=True` to force MadGraph to be started with Python 2.x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.run(\n",
    "    sample_benchmark='sm',\n",
    "    mg_directory=mg_dir,\n",
    "    mg_process_directory='./mg_processes/signal',\n",
    "    proc_card_file='cards/proc_card_signal.dat',\n",
    "    param_card_template_file='cards/param_card_template.dat',\n",
    "    run_card_file='cards/run_card_signal.dat',\n",
    "    log_directory='logs/signal',\n",
    "    python2_override=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a moment -- time for a coffee break!\n",
    "\n",
    "After running any event generation through MadMiner, you should check whether the run succeeded: are the usual output files there (LHE and HepMC), do the log files show any error messages? MadMiner does not perform any explicit checks, and if something went wrong in the event generation, it will only notice later when trying to load the event files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to start multiple processes based on the same `MadMiner` instance. This can be used to combine samples sampled according to different benchmarks, and to add reducible backgrounds. \n",
    "\n",
    "For the latter, a useful option is the `is_background` switch, which should be used for processes that do *not* depend on the parameters theta. `is_background=True` will disable the reweighting and re-use the same weights for all cross sections.\n",
    "\n",
    "To reduce the runtime of the notebook, the background part is commented out here. Feel free to activate it and let it run during a lunch break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "miner.run(\n",
    "    is_background=True,\n",
    "    sample_benchmark='sm',\n",
    "    mg_directory=mg_dir,\n",
    "    mg_process_directory='./mg_processes/background',\n",
    "    proc_card_file='cards/proc_card_background.dat',\n",
    "    param_card_template_file='cards/param_card_template.dat',\n",
    "    run_card_file='cards/run_card_background.dat',\n",
    "    log_directory='logs/background',\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two options might be very useful for larger projects:\n",
    "- `MadMiner.run_multiple()` allows you to start multiple runs with different run cards or different choices of `sample_benchmark`.\n",
    "- Both `MadMiner.run()` and `MadMiner.run_multiple()` have a `only_create_script` keyword. If that is set to True, MadMiner will not start the event generation directly, but prepare folders with all the right settings and ready-to-run bash scripts. This might make it much easier to generate Events on a high-performance computing system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run smearing and extract observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `madminer.lhe` submodule allows us to extract observables directly from the parton-level LHE samples, including an approximate description of the detector response with smearing functions. The central object is an instance of the `LHEProcessor` class, which has to be initialized with a MadMiner file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = LHEProcessor('data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the LHEProcessor object, one can add a number of event samples (the output of running MadGraph in step 3) with the `add_sample()` function.\n",
    "\n",
    "In addition, you have to provide the information which sample was generated from which benchmark with the `sampled_from_benchmark` keyword, and set `is_background=True` for all background samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "proc.add_sample(\n",
    "    lhe_filename='mg_processes/signal/Events/run_01/unweighted_events.lhe.gz',\n",
    "    sampled_from_benchmark='sm',\n",
    "    is_background=False,\n",
    "    k_factor=1.1,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "proc.add_sample(\n",
    "    lhe_filename='mg_processes/background/Events/run_01/unweighted_events.lhe.gz',\n",
    "    sampled_from_benchmark='sm',\n",
    "    is_background=True,\n",
    "    k_factor=1.0,\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to define the smearing functions that are used (in lieu of a proper shower and detector simulation). Here we will assume a simple 10% uncertainty on the jet energy measurements and a $\\pm 0.1$ smearing for jet $\\eta$ and $\\phi$. The transverse momenta of the jets are then derived from the smeared energy and the on-shell condition for the quarks (this is what `pt_resolution_abs=None` does). The photons from the Higgs are assumed to be measured perfectly (otherwise we'd have to call `set_smearing` another time with `pdgis=[22]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.set_smearing(\n",
    "    pdgids=[1,2,3,4,5,6,9,22,-1,-2,-3,-4,-5,-6],   # Partons giving rise to jets\n",
    "    energy_resolution_abs=0.,\n",
    "    energy_resolution_rel=0.1,\n",
    "    pt_resolution_abs=None,\n",
    "    pt_resolution_rel=None,\n",
    "    eta_resolution_abs=0.1,\n",
    "    eta_resolution_rel=0.,\n",
    "    phi_resolution_abs=0.1,\n",
    "    phi_resolution_rel=0.,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of observables, either through a Python function or an expression that can be evaluated. Here we demonstrate the latter, which is implemented in `add_observable()`. In the expression string, you can use the terms `j[i]`, `e[i]`, `mu[i]`, `a[i]`, `met`, where the indices `i` refer to a ordering by the transverse momentum. In addition, you can use `p[i]`, which denotes the `i`-th particle in the order given in the LHE sample (which is the order in which the final-state particles where defined in MadGraph).\n",
    "\n",
    "All of these represent objects inheriting from scikit-hep [LorentzVectors](http://scikit-hep.org/api/math.html#vector-classes), see the link for a documentation of their properties. In addition, they have `charge` and `pdg_id` properties.\n",
    "\n",
    "`add_observable()` has an optional keyword `required`. If `required=True`, we will only keep events where the observable can be parsed, i.e. all involved particles have been detected. If `required=False`, un-parseable observables will be filled with the value of another keyword `default`.\n",
    "\n",
    "In a realistic project, you would want to add a large number of observables that capture all information in your events. Here we will just define two observables, the transverse momentum of the leading (= higher-pT) jet, and the azimuthal angle between the two leading jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.add_observable(\n",
    "    'pt_j1',\n",
    "    'j[0].pt',\n",
    "    required=False,\n",
    "    default=0.,\n",
    ")\n",
    "proc.add_observable(\n",
    "    'delta_phi_jj',\n",
    "    'j[0].deltaphi(j[1]) * (-1. + 2.*float(j[0].eta > j[1].eta))',\n",
    "    required=True,\n",
    ")\n",
    "proc.add_observable(\n",
    "    'met',\n",
    "    'met.pt',\n",
    "    required=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add cuts, again in parse-able strings. In addition to the objects discussed above, they can contain the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.add_cut('(a[0] + a[1]).m > 124.')\n",
    "proc.add_cut('(a[0] + a[1]).m < 126.')\n",
    "proc.add_cut('pt_j1 > 30.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `analyse_samples` then calculates all observables from the LHE file(s) generated before, applies the smearing, and checks which events pass the cuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proc.analyse_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the observables and the weights are then saved in the HDF5 file. It is possible to overwrite the same file, or to leave the original file intact and save all the data into a new file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc.save('data/madminer_example_with_data2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our MC run produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_distributions(\n",
    "    filename='data/madminer_example_with_data2.h5',\n",
    "    parameter_points=['sm', np.array([10.,0.])],\n",
    "    line_labels=['SM', 'BSM'],\n",
    "    uncertainties='none',\n",
    "    n_bins=20,\n",
    "    n_cols=3,\n",
    "    normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine and shuffle different event samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce disk usage, you can generate several small event samples with the steps given above, and combine them now. Note that (for now) it is essential that all of them are generated with the same setup, including the same benchmark points / morphing basis!\n",
    "\n",
    "In our case we only have one sample, so this is not strictly necessary, but we still include it for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_and_shuffle(\n",
    "    ['data/madminer_example_with_data.h5'],\n",
    "    'data/madminer_example_shuffled.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the information we need from MadGraph, Pythia, and Delphes. But the data is not quite ready to be used for machine learning. The `madminer.sampling` class `SampleAugmenter` will take care of the remaining book-keeping steps before we can train our estimators:\n",
    "\n",
    "First, it unweights the samples, i.e. for a given parameter vector `theta` (or a distribution `p(theta)`) it picks events `x` such that their distribution follows `p(x|theta)`. The selected samples will all come from the event file we have so far, but their frequency is changed -- some events will appear multiple times, some will disappear.\n",
    "\n",
    "Second, `SampleAugmenter` calculates all the augmented data (\"gold\") that is the key to our new inference methods. Depending on the specific technique, these are the joint likelihood ratio and / or the joint score. It saves all these pieces of information for the selected events in a set of numpy files that can easily be used in any machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SampleAugmenter('data/madminer_example_shuffled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SampleAugmenter` class defines five different high-level functions to generate train or test samples:\n",
    "- `extract_samples_train_plain()`, which only saves observations x, for instance for histograms or ABC;\n",
    "- `extract_samples_train_local()` for methods like SALLY and SALLINO, which will be demonstrated in the second part of the tutorial;\n",
    "- `extract_samples_train_ratio()` for techniques like CARL, ROLR, CASCAL, and RASCAL, when only theta0 is parameterized;\n",
    "- `extract_samples_train_more_ratios()` for the same techniques, but with both theta0 and theta1 parameterized;\n",
    "- `extract_samples_test()` for the evaluation of any method.\n",
    "\n",
    "For the arguments `theta`, `theta0`, or `theta1`, you can (and should!) use the helper functions `constant_benchmark_theta()`, `multiple_benchmark_thetas()`, `constant_morphing_theta()`, `multiple_morphing_thetas()`, and `random_morphing_thetas()`, all defined in the `madminer.sampling` module.\n",
    "\n",
    "Here we'll train a likelihood ratio estimator with the ALICES method, so we focus on the `extract_samples_train_ratio()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta0, theta1, y, r_xz, t_xz = sa.extract_samples_train_ratio(\n",
    "    theta0=random_morphing_thetas(100, [('gaussian', 0., 10.), ('gaussian', 0., 10.)]),\n",
    "    theta1=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation we'll need a test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = sa.extract_samples_test(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cross section over parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate total cross sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_benchmarks, xsecs_benchmarks, xsec_errors_benchmarks = sa.extract_cross_sections(\n",
    "    theta=multiple_benchmark_thetas(['sm', 'w', 'morphing_basis_vector_2', 'morphing_basis_vector_3', 'morphing_basis_vector_4', 'morphing_basis_vector_5'])\n",
    ")\n",
    "\n",
    "thetas_morphing, xsecs_morphing, xsec_errors_morphing = sa.extract_cross_sections(\n",
    "    theta=random_morphing_thetas(1000, [('gaussian', 0., 4.), ('gaussian', 0., 4.)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin, cmax = 0., 2.5 * np.mean(xsecs_morphing)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "sc = plt.scatter(thetas_morphing[:,0], thetas_morphing[:,1], c=xsecs_morphing,\n",
    "            s=40., cmap='viridis', vmin=cmin, vmax=cmax,\n",
    "            marker='o')\n",
    "\n",
    "plt.scatter(thetas_benchmarks[:,0], thetas_benchmarks[:,1], c=xsecs_benchmarks,\n",
    "            s=200., cmap='viridis', vmin=cmin, vmax=cmax, lw=2., edgecolor='black',\n",
    "            marker='s')\n",
    "\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('xsec [pb]')\n",
    "\n",
    "plt.xlim(-10.,10.)\n",
    "plt.ylim(-10.,10.)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What  you see here is a morphing algorithm in action. We only asked MadGraph to calculate event weights (differential cross sections, or basically squared matrix elements) at six fixed parameter points (shown here as squares with black edges). But with our knowledge about the structure of the process we can interpolate any observable to any parameter point without loss (except that statistical uncertainties might increase)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train neural networks to estimate likelihood ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build the neural network that estimates the likelihood ratio. The central object for this is the `madminer.ml.MLForge` class. It defines functions that train, save, load, and evaluate the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the ALICES technique described in [\"Likelihood-free inference with an improved cross-entropy estimator\"](https://arxiv.org/abs/1808.00973). Most other methods, including RASCAL, are described in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). There is also SCANDAL introduced in [\"Mining gold from implicit models to improve likelihood-free inference\"](https://arxiv.org/abs/1805.12244).\n",
    "\n",
    "Most of these methods exist both in a \"single parameterized\" version, in which only the dependence of the likelihood ratio on the numerator is modelled, and a \"doubly parameterized\" version, in which both the dependence on the numerator and denominator parameters is modelled. For the single parameterized version, use `method='rascal'`, `method='alice'`, and so on. For the double parameterized version, use `method='rascal2'`, `method='alice2'`, etc. Note that for the doubly parameterized estimators you have to provide `theta1_filename`, and in the case of RASCAL and ALICES also `t_xz1_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:44 madminer.ml          INFO    Starting training\n",
      "11:44 madminer.ml          INFO      Method:                 alices\n",
      "11:44 madminer.ml          INFO      Training data:          x at data/samples/x_train.npy\n",
      "11:44 madminer.ml          INFO                              theta0 at data/samples/theta0_train.npy\n",
      "11:44 madminer.ml          INFO                              y at data/samples/y_train.npy\n",
      "11:44 madminer.ml          INFO                              r_xz at data/samples/r_xz_train.npy\n",
      "11:44 madminer.ml          INFO                              t_xz (theta0) at data/samples/t_xz_train.npy\n",
      "11:44 madminer.ml          INFO      Features:               all\n",
      "11:44 madminer.ml          INFO      Method:                 alices\n",
      "11:44 madminer.ml          INFO      Hidden layers:          (100, 100, 100)\n",
      "11:44 madminer.ml          INFO      Activation function:    tanh\n",
      "11:44 madminer.ml          INFO      alpha:                  10.0\n",
      "11:44 madminer.ml          INFO      Batch size:             200\n",
      "11:44 madminer.ml          INFO      Optimizer:              amsgrad\n",
      "11:44 madminer.ml          INFO      Epochs:                 50\n",
      "11:44 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "11:44 madminer.ml          INFO      Validation split:       0.25\n",
      "11:44 madminer.ml          INFO      Early stopping:         True\n",
      "11:44 madminer.ml          INFO      Scale inputs:           True\n",
      "11:44 madminer.ml          INFO      Shuffle labels          False\n",
      "11:44 madminer.ml          INFO      Samples:                all\n",
      "11:44 madminer.ml          INFO    Loading training data\n",
      "11:44 madminer.ml          INFO    Found 100000 samples with 2 parameters and 2 observables\n",
      "11:44 madminer.ml          INFO    Rescaling inputs\n",
      "11:44 madminer.ml          INFO    Creating model for method alices\n",
      "11:44 madminer.ml          INFO    Training model\n",
      "11:44 madminer.utils.ml.tr INFO    Epoch   1: train loss 2.168463 (improved_xe: 0.701064, mse_score: 0.146740)\n",
      "11:44 madminer.utils.ml.tr INFO               val. loss  1.963863 (improved_xe: 0.729682, mse_score: 0.123418)\n",
      "11:45 madminer.utils.ml.tr INFO    Epoch   2: train loss 2.035995 (improved_xe: 0.734035, mse_score: 0.130196)\n",
      "11:45 madminer.utils.ml.tr INFO               val. loss  1.841218 (improved_xe: 0.732819, mse_score: 0.110840)\n",
      "11:45 madminer.utils.ml.tr INFO    Epoch   3: train loss 1.930884 (improved_xe: 0.763404, mse_score: 0.116748)\n",
      "11:45 madminer.utils.ml.tr INFO               val. loss  1.793014 (improved_xe: 0.786990, mse_score: 0.100602)\n",
      "11:46 madminer.utils.ml.tr INFO    Epoch   4: train loss 1.855113 (improved_xe: 0.785439, mse_score: 0.106967)\n",
      "11:46 madminer.utils.ml.tr INFO               val. loss  1.709730 (improved_xe: 0.780750, mse_score: 0.092898)\n",
      "11:46 madminer.utils.ml.tr INFO    Epoch   5: train loss 1.796996 (improved_xe: 0.797276, mse_score: 0.099972)\n",
      "11:46 madminer.utils.ml.tr INFO               val. loss  1.677324 (improved_xe: 0.786557, mse_score: 0.089077)\n",
      "11:47 madminer.utils.ml.tr INFO    Epoch   6: train loss 1.746780 (improved_xe: 0.802705, mse_score: 0.094408)\n",
      "11:47 madminer.utils.ml.tr INFO               val. loss  1.664941 (improved_xe: 0.807695, mse_score: 0.085725)\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch   7: train loss 1.725325 (improved_xe: 0.810827, mse_score: 0.091450)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss  1.721082 (improved_xe: 0.782469, mse_score: 0.093861)\n",
      "11:48 madminer.utils.ml.tr INFO    Epoch   8: train loss 1.697415 (improved_xe: 0.793910, mse_score: 0.090351)\n",
      "11:48 madminer.utils.ml.tr INFO               val. loss  1.610422 (improved_xe: 0.801591, mse_score: 0.080883)\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch   9: train loss 1.657187 (improved_xe: 0.800058, mse_score: 0.085713)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss  1.591106 (improved_xe: 0.791988, mse_score: 0.079912)\n",
      "11:49 madminer.utils.ml.tr INFO    Epoch  10: train loss 1.649533 (improved_xe: 0.811406, mse_score: 0.083813)\n",
      "11:49 madminer.utils.ml.tr INFO               val. loss  1.592683 (improved_xe: 0.783339, mse_score: 0.080934)\n",
      "11:50 madminer.utils.ml.tr INFO    Epoch  11: train loss 1.633550 (improved_xe: 0.804845, mse_score: 0.082870)\n",
      "11:50 madminer.utils.ml.tr INFO               val. loss  1.583100 (improved_xe: 0.796092, mse_score: 0.078701)\n",
      "11:50 madminer.utils.ml.tr INFO    Epoch  12: train loss 1.610197 (improved_xe: 0.804405, mse_score: 0.080579)\n",
      "11:50 madminer.utils.ml.tr INFO               val. loss  1.561157 (improved_xe: 0.792704, mse_score: 0.076845)\n",
      "11:51 madminer.utils.ml.tr INFO    Epoch  13: train loss 1.597657 (improved_xe: 0.799357, mse_score: 0.079830)\n",
      "11:51 madminer.utils.ml.tr INFO               val. loss  1.554220 (improved_xe: 0.805682, mse_score: 0.074854)\n",
      "11:51 madminer.utils.ml.tr INFO    Epoch  14: train loss 1.594204 (improved_xe: 0.802237, mse_score: 0.079197)\n",
      "11:51 madminer.utils.ml.tr INFO               val. loss  1.560984 (improved_xe: 0.792825, mse_score: 0.076816)\n",
      "11:52 madminer.utils.ml.tr INFO    Epoch  15: train loss 1.584782 (improved_xe: 0.802131, mse_score: 0.078265)\n",
      "11:52 madminer.utils.ml.tr INFO               val. loss  1.562213 (improved_xe: 0.801670, mse_score: 0.076054)\n",
      "11:52 madminer.utils.ml.tr INFO    Epoch  16: train loss 1.563100 (improved_xe: 0.799750, mse_score: 0.076335)\n",
      "11:52 madminer.utils.ml.tr INFO               val. loss  1.543554 (improved_xe: 0.788312, mse_score: 0.075524)\n",
      "11:53 madminer.utils.ml.tr INFO    Epoch  17: train loss 1.570124 (improved_xe: 0.800298, mse_score: 0.076983)\n",
      "11:53 madminer.utils.ml.tr INFO               val. loss  1.535530 (improved_xe: 0.796940, mse_score: 0.073859)\n",
      "11:53 madminer.utils.ml.tr INFO    Epoch  18: train loss 1.545331 (improved_xe: 0.794775, mse_score: 0.075056)\n",
      "11:53 madminer.utils.ml.tr INFO               val. loss  1.534862 (improved_xe: 0.798901, mse_score: 0.073596)\n",
      "11:53 madminer.utils.ml.tr INFO    Epoch  19: train loss 1.529395 (improved_xe: 0.794652, mse_score: 0.073474)\n",
      "11:53 madminer.utils.ml.tr INFO               val. loss  1.530038 (improved_xe: 0.794381, mse_score: 0.073566)\n",
      "11:54 madminer.utils.ml.tr INFO    Epoch  20: train loss 1.518872 (improved_xe: 0.795222, mse_score: 0.072365)\n",
      "11:54 madminer.utils.ml.tr INFO               val. loss  1.519078 (improved_xe: 0.789876, mse_score: 0.072920)\n",
      "11:54 madminer.utils.ml.tr INFO    Epoch  21: train loss 1.521172 (improved_xe: 0.795041, mse_score: 0.072613)\n",
      "11:54 madminer.utils.ml.tr INFO               val. loss  1.531124 (improved_xe: 0.797651, mse_score: 0.073347)\n",
      "11:54 madminer.utils.ml.tr INFO    Epoch  22: train loss 1.505656 (improved_xe: 0.792785, mse_score: 0.071287)\n",
      "11:54 madminer.utils.ml.tr INFO               val. loss  1.530969 (improved_xe: 0.795749, mse_score: 0.073522)\n",
      "11:55 madminer.utils.ml.tr INFO    Epoch  23: train loss 1.507723 (improved_xe: 0.793440, mse_score: 0.071428)\n",
      "11:55 madminer.utils.ml.tr INFO               val. loss  1.519046 (improved_xe: 0.799723, mse_score: 0.071932)\n",
      "11:55 madminer.utils.ml.tr INFO    Epoch  24: train loss 1.502023 (improved_xe: 0.791717, mse_score: 0.071031)\n",
      "11:55 madminer.utils.ml.tr INFO               val. loss  1.511235 (improved_xe: 0.787877, mse_score: 0.072336)\n",
      "11:56 madminer.utils.ml.tr INFO    Epoch  25: train loss 1.489138 (improved_xe: 0.792146, mse_score: 0.069699)\n",
      "11:56 madminer.utils.ml.tr INFO               val. loss  1.509688 (improved_xe: 0.788501, mse_score: 0.072119)\n",
      "11:56 madminer.utils.ml.tr INFO    Epoch  26: train loss 1.487043 (improved_xe: 0.794145, mse_score: 0.069290)\n",
      "11:56 madminer.utils.ml.tr INFO               val. loss  1.562495 (improved_xe: 0.810636, mse_score: 0.075186)\n",
      "11:57 madminer.utils.ml.tr INFO    Epoch  27: train loss 1.498755 (improved_xe: 0.795410, mse_score: 0.070334)\n",
      "11:57 madminer.utils.ml.tr INFO               val. loss  1.505829 (improved_xe: 0.791182, mse_score: 0.071465)\n",
      "11:57 madminer.utils.ml.tr INFO    Epoch  28: train loss 1.479494 (improved_xe: 0.787403, mse_score: 0.069209)\n",
      "11:57 madminer.utils.ml.tr INFO               val. loss  1.504670 (improved_xe: 0.791773, mse_score: 0.071290)\n",
      "11:58 madminer.utils.ml.tr INFO    Epoch  29: train loss 1.468998 (improved_xe: 0.790482, mse_score: 0.067852)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:58 madminer.utils.ml.tr INFO               val. loss  1.504690 (improved_xe: 0.798024, mse_score: 0.070667)\n",
      "11:58 madminer.utils.ml.tr INFO    Epoch  30: train loss 1.465647 (improved_xe: 0.789190, mse_score: 0.067646)\n",
      "11:58 madminer.utils.ml.tr INFO               val. loss  1.506987 (improved_xe: 0.796184, mse_score: 0.071080)\n",
      "11:58 madminer.utils.ml.tr INFO    Epoch  31: train loss 1.465491 (improved_xe: 0.789613, mse_score: 0.067588)\n",
      "11:58 madminer.utils.ml.tr INFO               val. loss  1.503085 (improved_xe: 0.797587, mse_score: 0.070550)\n",
      "11:59 madminer.utils.ml.tr INFO    Epoch  32: train loss 1.461980 (improved_xe: 0.787123, mse_score: 0.067486)\n",
      "11:59 madminer.utils.ml.tr INFO               val. loss  1.498635 (improved_xe: 0.790798, mse_score: 0.070784)\n",
      "11:59 madminer.utils.ml.tr INFO    Epoch  33: train loss 1.452767 (improved_xe: 0.786789, mse_score: 0.066598)\n",
      "11:59 madminer.utils.ml.tr INFO               val. loss  1.498663 (improved_xe: 0.799752, mse_score: 0.069891)\n",
      "11:59 madminer.utils.ml.tr INFO    Epoch  34: train loss 1.448473 (improved_xe: 0.788854, mse_score: 0.065962)\n",
      "11:59 madminer.utils.ml.tr INFO               val. loss  1.492543 (improved_xe: 0.795114, mse_score: 0.069743)\n",
      "12:00 madminer.utils.ml.tr INFO    Epoch  35: train loss 1.446347 (improved_xe: 0.788416, mse_score: 0.065793)\n",
      "12:00 madminer.utils.ml.tr INFO               val. loss  1.496829 (improved_xe: 0.793292, mse_score: 0.070354)\n",
      "12:00 madminer.utils.ml.tr INFO    Epoch  36: train loss 1.450121 (improved_xe: 0.786889, mse_score: 0.066323)\n",
      "12:00 madminer.utils.ml.tr INFO               val. loss  1.484561 (improved_xe: 0.783673, mse_score: 0.070089)\n",
      "12:01 madminer.utils.ml.tr INFO    Epoch  37: train loss 1.441694 (improved_xe: 0.785578, mse_score: 0.065612)\n",
      "12:01 madminer.utils.ml.tr INFO               val. loss  1.490200 (improved_xe: 0.795861, mse_score: 0.069434)\n",
      "12:01 madminer.utils.ml.tr INFO    Epoch  38: train loss 1.436481 (improved_xe: 0.786743, mse_score: 0.064974)\n",
      "12:01 madminer.utils.ml.tr INFO               val. loss  1.488584 (improved_xe: 0.792187, mse_score: 0.069640)\n",
      "12:02 madminer.utils.ml.tr INFO    Epoch  39: train loss 1.437425 (improved_xe: 0.784574, mse_score: 0.065285)\n",
      "12:02 madminer.utils.ml.tr INFO               val. loss  1.496341 (improved_xe: 0.796095, mse_score: 0.070025)\n",
      "12:02 madminer.utils.ml.tr INFO    Epoch  40: train loss 1.432905 (improved_xe: 0.786380, mse_score: 0.064652)\n",
      "12:02 madminer.utils.ml.tr INFO               val. loss  1.483825 (improved_xe: 0.793047, mse_score: 0.069078)\n",
      "12:03 madminer.utils.ml.tr INFO    Epoch  41: train loss 1.427981 (improved_xe: 0.785898, mse_score: 0.064208)\n",
      "12:03 madminer.utils.ml.tr INFO               val. loss  1.483724 (improved_xe: 0.790537, mse_score: 0.069319)\n",
      "12:03 madminer.utils.ml.tr INFO    Epoch  42: train loss 1.429673 (improved_xe: 0.785942, mse_score: 0.064373)\n",
      "12:03 madminer.utils.ml.tr INFO               val. loss  1.528616 (improved_xe: 0.814224, mse_score: 0.071439)\n",
      "12:04 madminer.utils.ml.tr INFO    Epoch  43: train loss 1.428482 (improved_xe: 0.781685, mse_score: 0.064680)\n",
      "12:04 madminer.utils.ml.tr INFO               val. loss  1.479736 (improved_xe: 0.791448, mse_score: 0.068829)\n",
      "12:04 madminer.utils.ml.tr INFO    Epoch  44: train loss 1.422047 (improved_xe: 0.783885, mse_score: 0.063816)\n",
      "12:04 madminer.utils.ml.tr INFO               val. loss  1.484721 (improved_xe: 0.794877, mse_score: 0.068984)\n",
      "12:04 madminer.utils.ml.tr INFO    Epoch  45: train loss 1.420424 (improved_xe: 0.784049, mse_score: 0.063638)\n",
      "12:04 madminer.utils.ml.tr INFO               val. loss  1.482089 (improved_xe: 0.793244, mse_score: 0.068884)\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='alices',\n",
    "    theta0_filename='data/samples/theta0_train.npy',\n",
    "    x_filename='data/samples/x_train.npy',\n",
    "    y_filename='data/samples/y_train.npy',\n",
    "    r_xz_filename='data/samples/r_xz_train.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train.npy',\n",
    "    alpha=10.,\n",
    "    verbose=\"all\",\n",
    ")\n",
    "\n",
    "forge.save('models/alices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forge.evaluate(theta,x)` estimated the log likelihood ratio and the score for all combination between the given phase-space points `x` and parameters `theta`. That is, if given 100 events `x` and a grid of 25 `theta` points, it will return 25\\*100 estimates for the log likelihood and 25\\*100 estimates for the  score, both indexed by `[i_theta,i_x]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_each = np.linspace(-20.,20.,21)\n",
    "theta0, theta1 = np.meshgrid(theta_each, theta_each)\n",
    "theta_grid = np.vstack((theta0.flatten(), theta1.flatten())).T\n",
    "np.save('data/samples/theta_grid.npy', theta_grid)\n",
    "\n",
    "theta_denom = np.array([[0.,0.]])\n",
    "np.save('data/samples/theta_ref.npy', theta_denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.load('models/alices')\n",
    "\n",
    "log_r_hat, _, _ = forge.evaluate(\n",
    "    theta0_filename='data/samples/theta_grid.npy',\n",
    "    x='data/samples/x_test.npy',\n",
    "    evaluate_score=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = theta_each[1] - theta_each[0]\n",
    "edges = np.linspace(theta_each[0] - bin_size/2, theta_each[-1] + bin_size/2, len(theta_each)+1)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "expected_llr = np.mean(log_r_hat,axis=1)\n",
    "best_fit = theta_grid[np.argmin(-2.*expected_llr)]\n",
    "\n",
    "cmin, cmax = np.min(-2*expected_llr), np.max(-2*expected_llr)\n",
    "    \n",
    "pcm = ax.pcolormesh(edges, edges, -2. * expected_llr.reshape((21,21)),\n",
    "                    norm=matplotlib.colors.Normalize(vmin=cmin, vmax=cmax),\n",
    "                    cmap='viridis_r')\n",
    "cbar = fig.colorbar(pcm, ax=ax, extend='both')\n",
    "\n",
    "plt.scatter(best_fit[0], best_fit[1], s=80., color='black', marker='*')\n",
    "\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "cbar.set_label(r'$\\mathbb{E}_x [ -2\\, \\log \\,\\hat{r}(x | \\theta, \\theta_{SM}) ]$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this tutorial our sample size was very small, and the network does not really have a chance to converge to the correct likelihood ratio function. So don't worry if you find a minimum that is not at the right point (the SM, i.e. the origin in this plot). Feel free to dial up the event numbers in the run card as well as the training samples and see what happens then!\n",
    "\n",
    "That's it for now. Please have a look at the documentation for a detailed description of all classes and functions. And if you're curious about SALLY, Fisher information matrices, and ensemble methods, please look at the second part of the tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
