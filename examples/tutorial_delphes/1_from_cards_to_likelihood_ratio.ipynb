{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner tutorial 1: From cards to likelihood ratios\n",
    "\n",
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll introduce the basic MadMiner workflow. We'll show you how to use MadMiner to generate events, extract training data, and train neural networks to estimate likelihood ratios.\n",
    "\n",
    "This tutorial does not try to explain the inference methods. To understand what MadMiner is doing, please have a look at some papers first. In\n",
    "[\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013)\n",
    "we explain the basic idea of most of the methods presented here, while [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020) is an extensive 65-page handbook going through the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you execute this notebook, make sure you have running installations of MadGraph, Pythia, and Delphes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from madminer.core import MadMiner\n",
    "from madminer.delphes import DelphesProcessor\n",
    "from madminer.sampling import combine_and_shuffle\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import constant_benchmark_theta, multiple_benchmark_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge\n",
    "from madminer.plotting import plot_2d_morphing_basis, plot_distributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter here the path to your MG5 root directory. This notebook assumes that you installed Delphes and Pythia through MG5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MadMiner uses the Python `logging` module to provide additional information and debugging output. You can choose how much of this output you want to see by switching the level in the following lines to `logging.DEBUG` or `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example process, we'll simulate VBF Higgs production with a decay into two photons. We'll try to measure two EFT coefficients that affect this process.\n",
    "\n",
    "Have a look at the `cards` folder. You'll find text files (\"cards\") that specify the process simulation in typical MadGraph convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a `MadMiner` instance, the first important step is the definition of the parameter space. Each model parameter is characterized by a name as well as the LHA block and ID.\n",
    "\n",
    "If morphing is used, one also has to specify the maximal power with which the parameter contributes to the squared matrix element. For instance, a parameter that contributes only to one vertex, will typically have `morphing_max_power=2`, while a parameter that contributes to two vertices usually has `morphing_max_power=4`. Exceptions arise for instance when the interference effects between the SM and dimension-six operators are modelled, but the square of the dimension-six amplitude (subleading in 1/Lambda) is not taken into account, in which case `morphing_max_power=1`. The `parameter_range` argument defines the range of parameter values that are used for the automatic optimization of the morphing basis.\n",
    "\n",
    "Finally, the parameter values theta used internally by MadMiner and the parameter values written to the param_card (or reweight_card) given to MadGraph do not have to be exactly the same. With the option `parm_card_transform`, the user can supply a one-parameter function that maps a parameter value theta to the value given to MadGraph. This string is a python expression, in which `theta` is parsed as the parameter value. For instance, if the internal parameters are in the range (0, 1), but should be linearly scaled to (0, 100) in the param_card, one would have to use `param_card_transform=\"100*theta\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = MadMiner(debug=False)\n",
    "\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=2,\n",
    "    parameter_name='CWL2',\n",
    "    morphing_max_power=2,\n",
    "    param_card_transform=\"16.52*theta\",\n",
    "    parameter_range=(-10.,10.)\n",
    ")\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=5,\n",
    "    parameter_name='CPWL2',\n",
    "    morphing_max_power=2,\n",
    "    param_card_transform=\"16.52*theta\",\n",
    "    parameter_range=(-10.,10.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define benchmark points (evaluation points for |M|^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of all the points at which the weights (squared matrix elements) should be evaluated by MadGraph. We call these points \"benchmarks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set benchmarks by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define benchmarks by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.add_benchmark(\n",
    "    {'CWL2':0., 'CPWL2':0.},\n",
    "    'sm'\n",
    ")\n",
    "miner.add_benchmark(\n",
    "    {'CWL2':10., 'CPWL2':0.},\n",
    "    'w'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphing setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If morphing is used, the function `set_morphing` has to be called. With the option `include_existing_benchmarks=True`, MadMiner will keep all the benchmark points defined beforehand and run a simple optimization algorithm to fix the remaining ones for the basis (which may be none). Otherwise, MadMiner will optimize the full basis and forget about all previously defined benchmark points.\n",
    "\n",
    "The other important keyword is `max_overall_power`. This sets the maximal combined power with which all parameters together contribute to the squared matrix element (cross section). This constraint is in addition to the `morphing_max_power` keyword of `add_parameter()` (see above). For instance, if there are two parameters of interest `a` and `b`, and it is known that there are contributions proportional to `1` (constant), `a`, `a^2`, `b`, `b^2`, and `ab`, then `max_overall_power=2`. If there are also contributions proportional to `a^2 b`, `a b^2`, and `a^2 b^2`, then `max_overall_power=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "miner.set_morphing(\n",
    "    include_existing_benchmarks=True,\n",
    "    max_overall_power=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the resulting morphing basis and the \"morphing error\", i.e. the sum of squared morphing weights as a function of the parameter space. The black dots mark the benchmarks at which the squared matrix element will be evaluated when MadGraph is run. In between them, the morphing will interpolate. Since the morphing knows about the functional form of the amplitudes, there is only a small numerical uncertainty in this interpolation, we expect that the color in this plot is indicative of this uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_2d_morphing_basis(\n",
    "    miner.morpher,\n",
    "    xlabel=r'$c_{W} v^2 / \\Lambda^2$',\n",
    "    ylabel=r'$c_{\\tilde{W}} v^2 / \\Lambda^2$',\n",
    "    xrange=(-10.,10.),\n",
    "    yrange=(-10.,10.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space, benchmark points, and morphing setup are saved in a HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.save('data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can now be loaded again with `miner.load(filename)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save settings and run MadGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, MadMiner starts MadGraph and Pythia to generate events and calculate the weights. You have to provide paths to the process card, run card, param card (the entries corresponding to the parameters of interest will be automatically adapted), and an empty reweight card. Log files in the `log_directory` folder collect the MadGraph output and are important for debugging.\n",
    "\n",
    "The `sample_benchmark` option can be used to specify which benchmark should be used for sampling. If it is not used, MadMiner will automatically use the benchmark that was added first.\n",
    "\n",
    "Finally, if MadGraph is supposed to run in a different Python environment or requires other setup steps, you can use the `initial_command` argument. If your default Python environment is Python 2.7, you do not have to use this. If it is Python 3.x, you should use this to activate a Python 2 environment (see [the conda documentation](https://conda.io/docs/user-guide/tasks/manage-environments.html)). In the commented-out line we assume you have create a conda environment `python2` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.run(\n",
    "    sample_benchmark='sm',\n",
    "    mg_directory=mg_dir,\n",
    "    mg_process_directory='./mg_processes/signal',\n",
    "    proc_card_file='cards/proc_card_signal.dat',\n",
    "    param_card_template_file='cards/param_card_template.dat',\n",
    "    run_card_file='cards/run_card_signal.dat',\n",
    "    pythia8_card_file='cards/pythia8_card.dat',\n",
    "    log_directory='logs/signal',\n",
    "    # initial_command='source activate python2',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take a moment -- time for a coffee break!\n",
    "\n",
    "After running any event generation through MadMiner, you should check whether the run succeeded: are the usual output files there (LHE and HepMC), do the log files show any error messages? MadMiner does not perform any explicit checks, and if something went wrong in the event generation, it will only notice later when trying to load the event files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to start multiple processes based on the same `MadMiner` instance. This can be used to combine samples sampled according to different benchmarks, and to add reducible backgrounds. \n",
    "\n",
    "For the latter, a useful option is the `is_background` switch, which should be used for processes that do *not* depend on the parameters theta. `is_background=True` will disable the reweighting and re-use the same weights for all cross sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.run(\n",
    "    is_background=True,\n",
    "    sample_benchmark='sm',\n",
    "    mg_directory=mg_dir,\n",
    "    mg_process_directory='./mg_processes/background',\n",
    "    proc_card_file='cards/proc_card_background.dat',\n",
    "    param_card_template_file='cards/param_card_template.dat',\n",
    "    run_card_file='cards/run_card_background.dat',\n",
    "    pythia8_card_file='cards/pythia8_card.dat',\n",
    "    log_directory='logs/background',\n",
    "    # initial_command='source activate python2',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two options might be very useful for larger projects:\n",
    "- `MadMiner.run_multiple()` allows you to start multiple runs with different run cards or different choices of `sample_benchmark`.\n",
    "- Both `MadMiner.run()` and `MadMiner.run_multiple()` have a `only_create_script` keyword. If that is set to True, MadMiner will not start the event generation directly, but prepare folders with all the right settings and ready-to-run bash scripts. This might make it much easier to generate Events on a high-performance computing system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run detector simulation and extract observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `madminer.delphes` wraps around Delphes, a popular fast detector simulation. In addition to simulating the detector, it allows for the fast extraction of observables, which are saved in the MadMiner HDF5 file. The central object is an instance of the `DelphesProcessor` class, which has to be initialized with a MadMiner file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DelphesProcessor('data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the DelphesProcessor object, one can add a number of event samples (the output of running MadGraph and Pythia in step 3) with the `add_sample()` function.\n",
    "\n",
    "Currently, there is are issues with the MadGraph-Pythia interface and Delphes that force you to either install a manual patch (which will probably break with the next MadGraph update), or to provide *both* LHE and HepMC file for each sample as well as the keyword `weights='lhe'`. We hope that this won't be necessary in the future anymore.\n",
    "\n",
    "In addition, you have to provide the information which sample was generated from which benchmark with the `sampled_from_benchmark` keyword, and set `is_background=True` for all background samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dp.add_sample(\n",
    "    lhe_filename='mg_processes/signal/Events/run_01/unweighted_events.lhe.gz',\n",
    "    hepmc_filename='mg_processes/signal/Events/run_01/tag_1_pythia8_events.hepmc.gz',\n",
    "    sampled_from_benchmark='sm',\n",
    "    is_background=False,\n",
    "    weights='lhe',\n",
    ")\n",
    "dp.add_sample(\n",
    "    lhe_filename='mg_processes/background/Events/run_01/unweighted_events.lhe.gz',\n",
    "    hepmc_filename='mg_processes/background/Events/run_01/tag_1_pythia8_events.hepmc.gz',\n",
    "    sampled_from_benchmark='sm',\n",
    "    is_background=True,\n",
    "    weights='lhe',\n",
    ")\n",
    "\n",
    "dp.run_delphes(\n",
    "    delphes_directory=mg_dir + '/Delphes',\n",
    "    delphes_card='cards/delphes_card.dat',\n",
    "    log_file='logs/delphes.log',\n",
    "    # initial_command='source activate python2',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of observables, either through a Python function or an expression that can be evaluated. Here we demonstrate the latter, which is implemented in `add_observable()`. In the expression string, you can use the terms `j[i]`, `e[i]`, `mu[i]`, `a[i]`, `met`, where the indices `i` refer to a ordering by the transverse momentum. All of these represent objects inheriting from scikit-hep [LorentzVectors](http://scikit-hep.org/api/math.html#vector-classes), see the link for a documentation of their properties. In addition, they have `charge` and `pdg_id` properties.\n",
    "\n",
    "`add_observable()` has an optional keyword `required`. If `required=True`, we will only keep events where the observable can be parsed, i.e. all involved particles have been detected. If `required=False`, un-parseable observables will be filled with the value of another keyword `default`.\n",
    "\n",
    "In a realistic project, you would want to add a large number of observables that capture all information in your events. Here we will just define two observables, the transverse momentum of the leading (= higher-pT) jet, and the azimuthal angle between the two leading jets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.add_observable(\n",
    "    'pt_j1',\n",
    "    'j[0].pt',\n",
    "    required=False,\n",
    "    default=0.,\n",
    ")\n",
    "dp.add_observable(\n",
    "    'delta_phi_jj',\n",
    "    '(j[0].phi() - j[1].phi()) * (-1. + 2.*float(j[0].eta > j[1].eta))',\n",
    "    required=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add cuts, again in parse-able strings. In addition to the objects discussed above, they can contain the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.add_cut('pt_j1 > 30.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `analyse_delphes_samples` then calculates all observables from the Delphes ROOT file(s) generated before and applies the cuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.analyse_delphes_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the observables and the weights are then saved in the HDF5 file. It is possible to overwrite the same file, or to leave the original file intact and save all the data into a new file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.save('data/madminer_example_with_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One side remark: For the detector simulation and calculation of observables, different users might have very different requirements. While a phenomenologist might be content with the fast detector simulation from Delphes, an experimental analysis might require the full simulation through Geant4. We therefore intend this part to be interchangeable: any user should feel free to run the detector simulation of their choice and save the calculated observables in the MadMiner HDF5 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A look at distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our MC run produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_distributions(\n",
    "    filename='data/madminer_example_with_data.h5',\n",
    "    parameter_points=['sm', np.array([5.,0.])],\n",
    "    line_labels=['SM', 'BSM'],\n",
    "    uncertainties='none',\n",
    "    n_bins=20,\n",
    "    n_cols=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine and shuffle different event samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce disk usage, you can generate several small event samples with the steps given above, and combine them now. Note that (for now) it is essential that all of them are generated with the same setup, including the same benchmark points / morphing basis!\n",
    "\n",
    "In our case we only have one sample, so this is not strictly necessary, but we still include it for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_and_shuffle(\n",
    "    ['data/madminer_example_with_data.h5'],\n",
    "    'data/madminer_example_shuffled.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the information we need from MadGraph, Pythia, and Delphes. But the data is not quite ready to be used for machine learning. The `madminer.sampling` class `SampleAugmenter` will take care of the remaining book-keeping steps before we can train our estimators:\n",
    "\n",
    "First, it unweights the samples, i.e. for a given parameter vector `theta` (or a distribution `p(theta)`) it picks events `x` such that their distribution follows `p(x|theta)`. The selected samples will all come from the event file we have so far, but their frequency is changed -- some events will appear multiple times, some will disappear.\n",
    "\n",
    "Second, `SampleAugmenter` calculates all the augmented data (\"gold\") that is the key to our new inference methods. Depending on the specific technique, these are the joint likelihood ratio and / or the joint score. It saves all these pieces of information for the selected events in a set of numpy files that can easily be used in any machine learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SampleAugmenter('data/madminer_example_shuffled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SampleAugmenter` class defines five different high-level functions to generate train or test samples:\n",
    "- `extract_samples_train_plain()`, which only saves observations x, for instance for histograms or ABC;\n",
    "- `extract_samples_train_local()` for methods like SALLY and SALLINO, which will be demonstrated in the second part of the tutorial;\n",
    "- `extract_samples_train_ratio()` for techniques like CARL, ROLR, CASCAL, and RASCAL, when only theta0 is parameterized;\n",
    "- `extract_samples_train_more_ratios()` for the same techniques, but with both theta0 and theta1 parameterized;\n",
    "- `extract_samples_test()` for the evaluation of any method.\n",
    "\n",
    "For the arguments `theta`, `theta0`, or `theta1`, you can (and should!) use the helper functions `constant_benchmark_theta()`, `multiple_benchmark_thetas()`, `constant_morphing_theta()`, `multiple_morphing_thetas()`, and `random_morphing_thetas()`, all defined in the `madminer.sampling` module.\n",
    "\n",
    "Here we'll train a likelihood ratio estimator with the ALICES method, so we focus on the `extract_samples_train_ratio()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta0, theta1, y, r_xz, t_xz = sa.extract_samples_train_ratio(\n",
    "    theta0=random_morphing_thetas(100, [('gaussian', 0., 10.), ('gaussian', 0., 10.)]),\n",
    "    theta1=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='train'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation we'll need a test sample, and we'll make two just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = sa.extract_samples_test(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cross section over parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate total cross sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_benchmarks, xsecs_benchmarks, xsec_errors_benchmarks = sa.extract_cross_sections(\n",
    "    theta=multiple_benchmark_thetas(['sm', 'w', 'morphing_basis_vector_2', 'morphing_basis_vector_3', 'morphing_basis_vector_4', 'morphing_basis_vector_5'])\n",
    ")\n",
    "\n",
    "thetas_morphing, xsecs_morphing, xsec_errors_morphing = sa.extract_cross_sections(\n",
    "    theta=random_morphing_thetas(1000, [('gaussian', 0., 4.), ('gaussian', 0., 4.)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin, cmax = np.mean(xsecs_morphing) - 2 * np.std(xsecs_morphing), np.mean(xsecs_morphing) + 2 * np.std(xsecs_morphing)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "sc = plt.scatter(thetas_morphing[:,0], thetas_morphing[:,1], c=xsecs_morphing,\n",
    "            s=40., cmap='viridis', vmin=cmin, vmax=cmax,\n",
    "            marker='o')\n",
    "\n",
    "plt.scatter(thetas_benchmarks[:,0], thetas_benchmarks[:,1], c=xsecs_benchmarks,\n",
    "            s=200., cmap='viridis', vmin=cmin, vmax=cmax, lw=2., edgecolor='black',\n",
    "            marker='s')\n",
    "\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('xsec [pb]')\n",
    "\n",
    "plt.xlim(-10.,10.)\n",
    "plt.ylim(-10.,10.)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What  you see here is a morphing algorithm in action. We only asked MadGraph to calculate event weights (differential cross sections, or basically squared matrix elements) at six fixed parameter points (shown here as squares with black edges). But with our knowledge about the structure of the process we can interpolate any observable to any parameter point without loss (except that statistical uncertainties might increase)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train neural networks to estimate likelihood ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build the neural network that estimates the likelihood ratio. The central object for this is the `madminer.ml.MLForge` class. It defines functions that train, save, load, and evaluate the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the ALICES technique described in [\"Likelihood-free inference with an improved cross-entropy estimator\"](https://arxiv.org/abs/1808.00973). Most other methods, including RASCAL, are described in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). There is also SCANDAL introduced in [\"Mining gold from implicit models to improve likelihood-free inference\"](https://arxiv.org/abs/1805.12244).\n",
    "\n",
    "Most of these methods exist both in a \"single parameterized\" version, in which only the dependence of the likelihood ratio on the numerator is modelled, and a \"doubly parameterized\" version, in which both the dependence on the numerator and denominator parameters is modelled. For the single parameterized version, use `method='rascal'`, `method='alice'`, and so on. For the double parameterized version, use `method='rascal2'`, `method='alice2'`, etc. Note that for the doubly parameterized estimators you have to provide `theta1_filename`, and in the case of RASCAL and ALICES also `t_xz1_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.train(\n",
    "    method='alices',\n",
    "    theta0_filename='data/samples/theta0_train.npy',\n",
    "    x_filename='data/samples/x_train.npy',\n",
    "    y_filename='data/samples/y_train.npy',\n",
    "    r_xz_filename='data/samples/r_xz_train.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train.npy',\n",
    "    n_hidden=(20,20),\n",
    "    alpha=10.,\n",
    "    n_epochs=20,\n",
    "    validation_split=0.3,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "forge.save('models/alices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forge.evaluate(theta,x)` estimated the log likelihood ratio and the score for all combination between the given phase-space points `x` and parameters `theta`. That is, if given 100 events `x` and a grid of 25 `theta` points, it will return 25\\*100 estimates for the log likelihood and 25\\*100 estimates for the  score, both indexed by `[i_theta,i_x]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_each = np.linspace(-20.,20.,21)\n",
    "theta0, theta1 = np.meshgrid(theta_each, theta_each)\n",
    "theta_grid = np.vstack((theta0.flatten(), theta1.flatten())).T\n",
    "np.save('data/samples/theta_grid.npy', theta_grid)\n",
    "\n",
    "theta_denom = np.array([[0.,0.]])\n",
    "np.save('data/samples/theta_ref.npy', theta_denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.load('models/alices')\n",
    "\n",
    "log_r_hat, _, _ = forge.evaluate(\n",
    "    theta0_filename='data/samples/theta_grid.npy',\n",
    "    x='data/samples/x_test.npy',\n",
    "    evaluate_score=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = theta_each[1] - theta_each[0]\n",
    "edges = np.linspace(theta_each[0] - bin_size/2, theta_each[-1] + bin_size/2, len(theta_each)+1)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "expected_llr = np.mean(log_r_hat,axis=1)\n",
    "best_fit = theta_grid[np.argmin(-2.*expected_llr)]\n",
    "\n",
    "cmin, cmax = np.min(-2*expected_llr), np.max(-2*expected_llr)\n",
    "    \n",
    "pcm = ax.pcolormesh(edges, edges, -2. * expected_llr.reshape((21,21)),\n",
    "                    norm=matplotlib.colors.Normalize(vmin=cmin, vmax=cmax),\n",
    "                    cmap='viridis_r')\n",
    "cbar = fig.colorbar(pcm, ax=ax, extend='both')\n",
    "\n",
    "plt.scatter(best_fit[0], best_fit[1], s=80., color='black', marker='*')\n",
    "\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "cbar.set_label(r'$\\mathbb{E}_x [ -2\\, \\log \\,\\hat{r}(x | \\theta, \\theta_{SM}) ]$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this tutorial our sample size was very small, and the network does not really have a chance to converge to the correct likelihood ratio function. So don't worry if you find a minimum that is not at the right point (the SM, i.e. the origin in this plot). Feel free to dial up the event numbers in the run card as well as the training samples and see what happens then!\n",
    "\n",
    "That's it for now. Please have a look at the documentation for a detailed description of all classes and functions. And if you're curious about SALLY, Fisher information matrices, and ensemble methods, please look at the second part of the tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
