{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner Parton-Level Analysis for $W\\gamma$: Step 2 - Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018\n",
    "\n",
    "In this tutorial we'll demonstrate how to use MadMiner to generate train and test samples for the Information Geometry methods introduced in the following papers:\n",
    "- J. Brehmer, K. Cranmer, F. Kling, T. Plehn: [\"Better Higgs Measurements Through Information Geometry\"](https://arxiv.org/abs/1612.05261)\n",
    "- J. Brehmer, F. Kling, T. Plehn, T.M.P. Tait: [\"Better Higgs-CP Tests Through Information Geometry\"](https://arxiv.org/abs/1712.02350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "madminer_src_path = \"/Users/felixkling/Documents/GitHub/madminer\"\n",
    "sys.path.append(madminer_src_path)\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "% matplotlib inline\n",
    "\n",
    "from madminer.fisherinformation import FisherInformation\n",
    "from madminer.fisherinformation import project_information,profile_information\n",
    "\n",
    "from madminer.plotting import plot_fisher_information_contours_2d\n",
    "from madminer.plotting import plot_fisherinfo_barplot\n",
    "from madminer.plotting import kinematic_distribution_of_information\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import constant_benchmark_theta, multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Input File sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "usenamein='1M'\n",
    "usenameout='scoretest'\n",
    "nsamples=500000\n",
    "\n",
    "useinputdata = 'data/madminer_wgamma_xxx_observables_'+usenamein+'.h5'\n",
    "usesamplesdir = 'data/samples_'+usenameout+'/'\n",
    "usemodelsdir = 'models/samples_'+usenameout+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:50  \n",
      "20:50  ------------------------------------------------------------\n",
      "20:50  |                                                          |\n",
      "20:50  |  MadMiner v2018.11.06                                    |\n",
      "20:50  |                                                          |\n",
      "20:50  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "20:50  |                                                          |\n",
      "20:50  ------------------------------------------------------------\n",
      "20:50  \n",
      "20:50  Loading data from data/madminer_wgamma_xxx_observables_1M.h5\n",
      "20:50  Found 2 parameters:\n",
      "20:50     CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "20:50     CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "20:50  Found 6 benchmarks:\n",
      "20:50     sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00\n",
      "20:50     w: CWL2 = 20.00, CPWL2 = 0.00e+00\n",
      "20:50     morphing_basis_vector_2: CWL2 = -4.69e+01, CPWL2 = 27.54\n",
      "20:50     morphing_basis_vector_3: CWL2 = 21.00, CPWL2 = 46.69\n",
      "20:50     morphing_basis_vector_4: CWL2 = 32.29, CPWL2 = -4.64e+01\n",
      "20:50     morphing_basis_vector_5: CWL2 = -3.43e+01, CPWL2 = -3.61e+01\n",
      "20:50  Found 23 observables: px_l, px_v, px_a, py_l, py_v, py_a, pz_l, pz_v, pz_a, e_l, e_v, e_a, pt_l, pt_v, pt_a, eta_l, eta_v, eta_a, dphi_lv, dphi_la, dphi_va, m_lv, m_lva\n",
      "20:50  Found 1000000 events\n",
      "20:50  Found morphing setup with 6 components\n",
      "20:50  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "20:50  Effective number of samples: 499999.99999981694\n",
      "20:50  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "20:50  Effective number of samples: 499999.99999981694\n",
      "20:50  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "20:50  Effective number of samples: 499999.99999981694\n",
      "20:50  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "20:50  Effective number of samples: 499999.99999981694\n",
      "20:50  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "20:50  Effective number of samples: 499999.99999981694\n",
      "20:50  Extracting evaluation sample. Sampling according to (u'benchmark', u'sm')\n",
      "20:50  Effective number of samples: 499998.9999998169\n"
     ]
    }
   ],
   "source": [
    "#create sample augmenter\n",
    "sa = SampleAugmenter(useinputdata, debug=False)\n",
    "n_estimators = 5\n",
    "\n",
    "#augment train sample\n",
    "for i in range(n_estimators):\n",
    "    x, theta, t_xz = sa.extract_samples_train_local(\n",
    "        theta=constant_benchmark_theta('sm'),\n",
    "        n_samples=nsamples,\n",
    "        folder='./'+usesamplesdir,\n",
    "        filename='train{}'.format(i)\n",
    "    )\n",
    "\n",
    "#augment test sample\n",
    "x, theta = sa.extract_samples_test(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=nsamples,\n",
    "    folder='./'+usesamplesdir,\n",
    "    filename='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a neural network to estimate the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a) Change Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:50  Training 5 estimators in ensemble\n",
      "20:50  Training estimator 1 / 5 in ensemble\n",
      "20:50  Starting training\n",
      "20:50    Method:                 sally\n",
      "20:50    Training data: x at data/samples_scoretest/x_train0.npy\n",
      "20:50                   t_xz (theta0) at  data/samples_scoretest/t_xz_train0.npy\n",
      "20:50    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8)\n",
      "20:50    Method:                 sally\n",
      "20:50    Hidden layers:          (100, 100)\n",
      "20:50    Activation function:    tanh\n",
      "20:50    Batch size:             128\n",
      "20:50    Trainer:                amsgrad\n",
      "20:50    Epochs:                 50\n",
      "20:50    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:50    Validation split:       None\n",
      "20:50    Early stopping:         True\n",
      "20:50    Scale inputs:           True\n",
      "20:50    Regularization:         None\n",
      "20:50  Loading training data\n",
      "20:50  Found 500000 samples with 2 parameters and 23 observables\n",
      "20:50  Rescaling inputs\n",
      "20:50  Only using 9 of 23 observables\n",
      "20:50  Creating model for method sally\n",
      "20:50  Training model\n",
      "20:51    Epoch 5: train loss 0.0007 (mse_score: 0.0007)\n",
      "20:52    Epoch 10: train loss 0.0006 (mse_score: 0.0006)\n",
      "20:52    Epoch 15: train loss 0.0005 (mse_score: 0.0005)\n",
      "20:53    Epoch 20: train loss 0.0005 (mse_score: 0.0005)\n",
      "20:54    Epoch 25: train loss 0.0005 (mse_score: 0.0005)\n",
      "20:54    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "20:55    Epoch 35: train loss 0.0004 (mse_score: 0.0004)\n",
      "20:56    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "20:57    Epoch 45: train loss 0.0004 (mse_score: 0.0004)\n",
      "20:57    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "20:57  Finished training\n",
      "20:57  Training estimator 2 / 5 in ensemble\n",
      "20:57  Starting training\n",
      "20:57    Method:                 sally\n",
      "20:57    Training data: x at data/samples_scoretest/x_train1.npy\n",
      "20:57                   t_xz (theta0) at  data/samples_scoretest/t_xz_train1.npy\n",
      "20:57    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8)\n",
      "20:57    Method:                 sally\n",
      "20:57    Hidden layers:          (100, 100)\n",
      "20:57    Activation function:    tanh\n",
      "20:57    Batch size:             128\n",
      "20:57    Trainer:                amsgrad\n",
      "20:57    Epochs:                 50\n",
      "20:57    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "20:57    Validation split:       None\n",
      "20:57    Early stopping:         True\n",
      "20:57    Scale inputs:           True\n",
      "20:57    Regularization:         None\n",
      "20:57  Loading training data\n",
      "20:57  Found 500000 samples with 2 parameters and 23 observables\n",
      "20:57  Rescaling inputs\n",
      "20:57  Only using 9 of 23 observables\n",
      "20:57  Creating model for method sally\n",
      "20:57  Training model\n",
      "20:58    Epoch 5: train loss 0.0007 (mse_score: 0.0007)\n",
      "20:59    Epoch 10: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:00    Epoch 15: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:00    Epoch 20: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:01    Epoch 25: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:02    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:03    Epoch 35: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:03    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:04    Epoch 45: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:05    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:05  Finished training\n",
      "21:05  Training estimator 3 / 5 in ensemble\n",
      "21:05  Starting training\n",
      "21:05    Method:                 sally\n",
      "21:05    Training data: x at data/samples_scoretest/x_train2.npy\n",
      "21:05                   t_xz (theta0) at  data/samples_scoretest/t_xz_train2.npy\n",
      "21:05    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8)\n",
      "21:05    Method:                 sally\n",
      "21:05    Hidden layers:          (100, 100)\n",
      "21:05    Activation function:    tanh\n",
      "21:05    Batch size:             128\n",
      "21:05    Trainer:                amsgrad\n",
      "21:05    Epochs:                 50\n",
      "21:05    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:05    Validation split:       None\n",
      "21:05    Early stopping:         True\n",
      "21:05    Scale inputs:           True\n",
      "21:05    Regularization:         None\n",
      "21:05  Loading training data\n",
      "21:05  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:05  Rescaling inputs\n",
      "21:05  Only using 9 of 23 observables\n",
      "21:05  Creating model for method sally\n",
      "21:05  Training model\n",
      "21:06    Epoch 5: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:06    Epoch 10: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:07    Epoch 15: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:08    Epoch 20: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:08    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:09    Epoch 30: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:10    Epoch 35: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:11    Epoch 40: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:11    Epoch 45: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:12    Epoch 50: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:12  Finished training\n",
      "21:12  Training estimator 4 / 5 in ensemble\n",
      "21:12  Starting training\n",
      "21:12    Method:                 sally\n",
      "21:12    Training data: x at data/samples_scoretest/x_train3.npy\n",
      "21:12                   t_xz (theta0) at  data/samples_scoretest/t_xz_train3.npy\n",
      "21:12    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8)\n",
      "21:12    Method:                 sally\n",
      "21:12    Hidden layers:          (100, 100)\n",
      "21:12    Activation function:    tanh\n",
      "21:12    Batch size:             128\n",
      "21:12    Trainer:                amsgrad\n",
      "21:12    Epochs:                 50\n",
      "21:12    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:12    Validation split:       None\n",
      "21:12    Early stopping:         True\n",
      "21:12    Scale inputs:           True\n",
      "21:12    Regularization:         None\n",
      "21:12  Loading training data\n",
      "21:12  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:12  Rescaling inputs\n",
      "21:12  Only using 9 of 23 observables\n",
      "21:12  Creating model for method sally\n",
      "21:12  Training model\n",
      "21:13    Epoch 5: train loss 0.0007 (mse_score: 0.0007)\n",
      "21:14    Epoch 10: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:14    Epoch 15: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:15    Epoch 20: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:16    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:17    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:17    Epoch 35: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:18    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:19    Epoch 45: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:20    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:20  Finished training\n",
      "21:20  Training estimator 5 / 5 in ensemble\n",
      "21:20  Starting training\n",
      "21:20    Method:                 sally\n",
      "21:20    Training data: x at data/samples_scoretest/x_train4.npy\n",
      "21:20                   t_xz (theta0) at  data/samples_scoretest/t_xz_train4.npy\n",
      "21:20    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8)\n",
      "21:20    Method:                 sally\n",
      "21:20    Hidden layers:          (100, 100)\n",
      "21:20    Activation function:    tanh\n",
      "21:20    Batch size:             128\n",
      "21:20    Trainer:                amsgrad\n",
      "21:20    Epochs:                 50\n",
      "21:20    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:20    Validation split:       None\n",
      "21:20    Early stopping:         True\n",
      "21:20    Scale inputs:           True\n",
      "21:20    Regularization:         None\n",
      "21:20  Loading training data\n",
      "21:20  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:20  Rescaling inputs\n",
      "21:20  Only using 9 of 23 observables\n",
      "21:20  Creating model for method sally\n",
      "21:20  Training model\n",
      "21:21    Epoch 5: train loss 0.0007 (mse_score: 0.0007)\n",
      "21:21    Epoch 10: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:22    Epoch 15: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:23    Epoch 20: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:24    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:24    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:25    Epoch 35: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:26    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:27    Epoch 45: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:27    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:27  Finished training\n",
      "21:27  Training 5 estimators in ensemble\n",
      "21:27  Training estimator 1 / 5 in ensemble\n",
      "21:27  Starting training\n",
      "21:27    Method:                 sally\n",
      "21:27    Training data: x at data/samples_scoretest/x_train0.npy\n",
      "21:27                   t_xz (theta0) at  data/samples_scoretest/t_xz_train0.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:27    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
      "21:27    Method:                 sally\n",
      "21:27    Hidden layers:          (100, 100)\n",
      "21:27    Activation function:    tanh\n",
      "21:27    Batch size:             128\n",
      "21:27    Trainer:                amsgrad\n",
      "21:27    Epochs:                 50\n",
      "21:27    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:27    Validation split:       None\n",
      "21:27    Early stopping:         True\n",
      "21:27    Scale inputs:           True\n",
      "21:27    Regularization:         None\n",
      "21:27  Loading training data\n",
      "21:27  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:27  Rescaling inputs\n",
      "21:27  Only using 12 of 23 observables\n",
      "21:27  Creating model for method sally\n",
      "21:27  Training model\n",
      "21:28    Epoch 5: train loss 0.0007 (mse_score: 0.0007)\n",
      "21:29    Epoch 10: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:29    Epoch 15: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:30    Epoch 20: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:31    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:32    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:32    Epoch 35: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:33    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:34    Epoch 45: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:35    Epoch 50: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:35  Finished training\n",
      "21:35  Training estimator 2 / 5 in ensemble\n",
      "21:35  Starting training\n",
      "21:35    Method:                 sally\n",
      "21:35    Training data: x at data/samples_scoretest/x_train1.npy\n",
      "21:35                   t_xz (theta0) at  data/samples_scoretest/t_xz_train1.npy\n",
      "21:35    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
      "21:35    Method:                 sally\n",
      "21:35    Hidden layers:          (100, 100)\n",
      "21:35    Activation function:    tanh\n",
      "21:35    Batch size:             128\n",
      "21:35    Trainer:                amsgrad\n",
      "21:35    Epochs:                 50\n",
      "21:35    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:35    Validation split:       None\n",
      "21:35    Early stopping:         True\n",
      "21:35    Scale inputs:           True\n",
      "21:35    Regularization:         None\n",
      "21:35  Loading training data\n",
      "21:35  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:35  Rescaling inputs\n",
      "21:35  Only using 12 of 23 observables\n",
      "21:35  Creating model for method sally\n",
      "21:35  Training model\n",
      "21:36    Epoch 5: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:36    Epoch 10: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:37    Epoch 15: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:38    Epoch 20: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:39    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:39    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:40    Epoch 35: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:41    Epoch 40: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:41    Epoch 45: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:42    Epoch 50: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:42  Finished training\n",
      "21:42  Training estimator 3 / 5 in ensemble\n",
      "21:42  Starting training\n",
      "21:42    Method:                 sally\n",
      "21:42    Training data: x at data/samples_scoretest/x_train2.npy\n",
      "21:42                   t_xz (theta0) at  data/samples_scoretest/t_xz_train2.npy\n",
      "21:42    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
      "21:42    Method:                 sally\n",
      "21:42    Hidden layers:          (100, 100)\n",
      "21:42    Activation function:    tanh\n",
      "21:42    Batch size:             128\n",
      "21:42    Trainer:                amsgrad\n",
      "21:42    Epochs:                 50\n",
      "21:42    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:42    Validation split:       None\n",
      "21:42    Early stopping:         True\n",
      "21:42    Scale inputs:           True\n",
      "21:42    Regularization:         None\n",
      "21:42  Loading training data\n",
      "21:42  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:42  Rescaling inputs\n",
      "21:42  Only using 12 of 23 observables\n",
      "21:42  Creating model for method sally\n",
      "21:42  Training model\n",
      "21:43    Epoch 5: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:44    Epoch 10: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:44    Epoch 15: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:45    Epoch 20: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:46    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:47    Epoch 30: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:47    Epoch 35: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:48    Epoch 40: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:49    Epoch 45: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:49    Epoch 50: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:49  Finished training\n",
      "21:49  Training estimator 4 / 5 in ensemble\n",
      "21:49  Starting training\n",
      "21:49    Method:                 sally\n",
      "21:49    Training data: x at data/samples_scoretest/x_train3.npy\n",
      "21:49                   t_xz (theta0) at  data/samples_scoretest/t_xz_train3.npy\n",
      "21:49    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
      "21:49    Method:                 sally\n",
      "21:49    Hidden layers:          (100, 100)\n",
      "21:49    Activation function:    tanh\n",
      "21:49    Batch size:             128\n",
      "21:49    Trainer:                amsgrad\n",
      "21:49    Epochs:                 50\n",
      "21:49    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:49    Validation split:       None\n",
      "21:49    Early stopping:         True\n",
      "21:49    Scale inputs:           True\n",
      "21:49    Regularization:         None\n",
      "21:49  Loading training data\n",
      "21:49  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:49  Rescaling inputs\n",
      "21:49  Only using 12 of 23 observables\n",
      "21:49  Creating model for method sally\n",
      "21:49  Training model\n",
      "21:50    Epoch 5: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:51    Epoch 10: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:52    Epoch 15: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:52    Epoch 20: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:53    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "21:54    Epoch 30: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:55    Epoch 35: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:55    Epoch 40: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:56    Epoch 45: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:57    Epoch 50: train loss 0.0003 (mse_score: 0.0003)\n",
      "21:57  Finished training\n",
      "21:57  Training estimator 5 / 5 in ensemble\n",
      "21:57  Starting training\n",
      "21:57    Method:                 sally\n",
      "21:57    Training data: x at data/samples_scoretest/x_train4.npy\n",
      "21:57                   t_xz (theta0) at  data/samples_scoretest/t_xz_train4.npy\n",
      "21:57    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\n",
      "21:57    Method:                 sally\n",
      "21:57    Hidden layers:          (100, 100)\n",
      "21:57    Activation function:    tanh\n",
      "21:57    Batch size:             128\n",
      "21:57    Trainer:                amsgrad\n",
      "21:57    Epochs:                 50\n",
      "21:57    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "21:57    Validation split:       None\n",
      "21:57    Early stopping:         True\n",
      "21:57    Scale inputs:           True\n",
      "21:57    Regularization:         None\n",
      "21:57  Loading training data\n",
      "21:57  Found 500000 samples with 2 parameters and 23 observables\n",
      "21:57  Rescaling inputs\n",
      "21:57  Only using 12 of 23 observables\n",
      "21:57  Creating model for method sally\n",
      "21:57  Training model\n",
      "21:58    Epoch 5: train loss 0.0006 (mse_score: 0.0006)\n",
      "21:58    Epoch 10: train loss 0.0005 (mse_score: 0.0005)\n",
      "21:59    Epoch 15: train loss 0.0005 (mse_score: 0.0005)\n",
      "22:00    Epoch 20: train loss 0.0004 (mse_score: 0.0004)\n",
      "22:01    Epoch 25: train loss 0.0004 (mse_score: 0.0004)\n",
      "22:01    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "22:02    Epoch 35: train loss 0.0003 (mse_score: 0.0003)\n",
      "22:03    Epoch 40: train loss 0.0003 (mse_score: 0.0003)\n",
      "22:03    Epoch 45: train loss 0.0003 (mse_score: 0.0003)\n",
      "22:04    Epoch 50: train loss 0.0003 (mse_score: 0.0003)\n",
      "22:04  Finished training\n",
      "22:04  Training 5 estimators in ensemble\n",
      "22:04  Training estimator 1 / 5 in ensemble\n",
      "22:04  Starting training\n",
      "22:04    Method:                 sally\n",
      "22:04    Training data: x at data/samples_scoretest/x_train0.npy\n",
      "22:04                   t_xz (theta0) at  data/samples_scoretest/t_xz_train0.npy\n",
      "22:04    Features:               (12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:04    Method:                 sally\n",
      "22:04    Hidden layers:          (100, 100)\n",
      "22:04    Activation function:    tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:04    Batch size:             128\n",
      "22:04    Trainer:                amsgrad\n",
      "22:04    Epochs:                 50\n",
      "22:04    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:04    Validation split:       None\n",
      "22:04    Early stopping:         True\n",
      "22:04    Scale inputs:           True\n",
      "22:04    Regularization:         None\n",
      "22:04  Loading training data\n",
      "22:04  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:04  Rescaling inputs\n",
      "22:04  Only using 11 of 23 observables\n",
      "22:04  Creating model for method sally\n",
      "22:04  Training model\n",
      "22:05    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:06    Epoch 10: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:06    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:07    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:08    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:08    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:09    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:10    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:10    Epoch 45: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:11    Epoch 50: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:11  Finished training\n",
      "22:11  Training estimator 2 / 5 in ensemble\n",
      "22:11  Starting training\n",
      "22:11    Method:                 sally\n",
      "22:11    Training data: x at data/samples_scoretest/x_train1.npy\n",
      "22:11                   t_xz (theta0) at  data/samples_scoretest/t_xz_train1.npy\n",
      "22:11    Features:               (12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:11    Method:                 sally\n",
      "22:11    Hidden layers:          (100, 100)\n",
      "22:11    Activation function:    tanh\n",
      "22:11    Batch size:             128\n",
      "22:11    Trainer:                amsgrad\n",
      "22:11    Epochs:                 50\n",
      "22:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:11    Validation split:       None\n",
      "22:11    Early stopping:         True\n",
      "22:11    Scale inputs:           True\n",
      "22:11    Regularization:         None\n",
      "22:11  Loading training data\n",
      "22:11  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:11  Rescaling inputs\n",
      "22:11  Only using 11 of 23 observables\n",
      "22:11  Creating model for method sally\n",
      "22:11  Training model\n",
      "22:12    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:13    Epoch 10: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:14    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:14    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:15    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:16    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:16    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:17    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:18    Epoch 45: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:18    Epoch 50: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:18  Finished training\n",
      "22:18  Training estimator 3 / 5 in ensemble\n",
      "22:18  Starting training\n",
      "22:18    Method:                 sally\n",
      "22:18    Training data: x at data/samples_scoretest/x_train2.npy\n",
      "22:18                   t_xz (theta0) at  data/samples_scoretest/t_xz_train2.npy\n",
      "22:18    Features:               (12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:18    Method:                 sally\n",
      "22:18    Hidden layers:          (100, 100)\n",
      "22:18    Activation function:    tanh\n",
      "22:18    Batch size:             128\n",
      "22:18    Trainer:                amsgrad\n",
      "22:18    Epochs:                 50\n",
      "22:18    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:18    Validation split:       None\n",
      "22:18    Early stopping:         True\n",
      "22:18    Scale inputs:           True\n",
      "22:18    Regularization:         None\n",
      "22:18  Loading training data\n",
      "22:18  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:18  Rescaling inputs\n",
      "22:18  Only using 11 of 23 observables\n",
      "22:18  Creating model for method sally\n",
      "22:18  Training model\n",
      "22:19    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:20    Epoch 10: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:21    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:21    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:22    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:23    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:23    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:24    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:25    Epoch 45: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:25    Epoch 50: train loss 0.0000 (mse_score: 0.0000)\n",
      "22:25  Finished training\n",
      "22:25  Training estimator 4 / 5 in ensemble\n",
      "22:25  Starting training\n",
      "22:25    Method:                 sally\n",
      "22:25    Training data: x at data/samples_scoretest/x_train3.npy\n",
      "22:25                   t_xz (theta0) at  data/samples_scoretest/t_xz_train3.npy\n",
      "22:25    Features:               (12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:25    Method:                 sally\n",
      "22:25    Hidden layers:          (100, 100)\n",
      "22:25    Activation function:    tanh\n",
      "22:25    Batch size:             128\n",
      "22:25    Trainer:                amsgrad\n",
      "22:25    Epochs:                 50\n",
      "22:25    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:25    Validation split:       None\n",
      "22:25    Early stopping:         True\n",
      "22:25    Scale inputs:           True\n",
      "22:25    Regularization:         None\n",
      "22:25  Loading training data\n",
      "22:25  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:25  Rescaling inputs\n",
      "22:25  Only using 11 of 23 observables\n",
      "22:25  Creating model for method sally\n",
      "22:25  Training model\n",
      "22:26    Epoch 5: train loss 0.0003 (mse_score: 0.0003)\n",
      "22:27    Epoch 10: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:28    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:28    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:29    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:30    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:30    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:31    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:32    Epoch 45: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:32    Epoch 50: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:32  Finished training\n",
      "22:32  Training estimator 5 / 5 in ensemble\n",
      "22:32  Starting training\n",
      "22:32    Method:                 sally\n",
      "22:32    Training data: x at data/samples_scoretest/x_train4.npy\n",
      "22:32                   t_xz (theta0) at  data/samples_scoretest/t_xz_train4.npy\n",
      "22:32    Features:               (12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:32    Method:                 sally\n",
      "22:32    Hidden layers:          (100, 100)\n",
      "22:32    Activation function:    tanh\n",
      "22:32    Batch size:             128\n",
      "22:32    Trainer:                amsgrad\n",
      "22:32    Epochs:                 50\n",
      "22:32    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:32    Validation split:       None\n",
      "22:32    Early stopping:         True\n",
      "22:32    Scale inputs:           True\n",
      "22:32    Regularization:         None\n",
      "22:32  Loading training data\n",
      "22:32  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:32  Rescaling inputs\n",
      "22:32  Only using 11 of 23 observables\n",
      "22:32  Creating model for method sally\n",
      "22:32  Training model\n",
      "22:33    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:34    Epoch 10: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:35    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:35    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:36    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:37    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:37    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:38    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:39    Epoch 45: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:39    Epoch 50: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:39  Finished training\n",
      "22:39  Training 5 estimators in ensemble\n",
      "22:39  Training estimator 1 / 5 in ensemble\n",
      "22:39  Starting training\n",
      "22:39    Method:                 sally\n",
      "22:39    Training data: x at data/samples_scoretest/x_train0.npy\n",
      "22:39                   t_xz (theta0) at  data/samples_scoretest/t_xz_train0.npy\n",
      "22:39    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:39    Method:                 sally\n",
      "22:39    Hidden layers:          (100, 100)\n",
      "22:39    Activation function:    tanh\n",
      "22:39    Batch size:             128\n",
      "22:39    Trainer:                amsgrad\n",
      "22:39    Epochs:                 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:39    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:39    Validation split:       None\n",
      "22:39    Early stopping:         True\n",
      "22:39    Scale inputs:           True\n",
      "22:39    Regularization:         None\n",
      "22:39  Loading training data\n",
      "22:39  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:39  Rescaling inputs\n",
      "22:39  Only using 23 of 23 observables\n",
      "22:39  Creating model for method sally\n",
      "22:39  Training model\n",
      "22:40    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:41    Epoch 10: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:42    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:42    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:43    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:44    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:45    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:46    Epoch 40: train loss 0.0000 (mse_score: 0.0000)\n",
      "22:47    Epoch 45: train loss 0.0000 (mse_score: 0.0000)\n",
      "22:47    Epoch 50: train loss 0.0000 (mse_score: 0.0000)\n",
      "22:47  Finished training\n",
      "22:47  Training estimator 2 / 5 in ensemble\n",
      "22:47  Starting training\n",
      "22:47    Method:                 sally\n",
      "22:47    Training data: x at data/samples_scoretest/x_train1.npy\n",
      "22:47                   t_xz (theta0) at  data/samples_scoretest/t_xz_train1.npy\n",
      "22:47    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:47    Method:                 sally\n",
      "22:47    Hidden layers:          (100, 100)\n",
      "22:47    Activation function:    tanh\n",
      "22:47    Batch size:             128\n",
      "22:47    Trainer:                amsgrad\n",
      "22:47    Epochs:                 50\n",
      "22:47    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:47    Validation split:       None\n",
      "22:47    Early stopping:         True\n",
      "22:47    Scale inputs:           True\n",
      "22:47    Regularization:         None\n",
      "22:47  Loading training data\n",
      "22:47  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:47  Rescaling inputs\n",
      "22:47  Only using 23 of 23 observables\n",
      "22:47  Creating model for method sally\n",
      "22:47  Training model\n",
      "22:49    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:50    Epoch 10: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:50    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:51    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:52    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:52    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:53    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:54    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:55    Epoch 45: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:55    Epoch 50: train loss 0.0000 (mse_score: 0.0000)\n",
      "22:55  Finished training\n",
      "22:55  Training estimator 3 / 5 in ensemble\n",
      "22:55  Starting training\n",
      "22:55    Method:                 sally\n",
      "22:55    Training data: x at data/samples_scoretest/x_train2.npy\n",
      "22:55                   t_xz (theta0) at  data/samples_scoretest/t_xz_train2.npy\n",
      "22:55    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "22:55    Method:                 sally\n",
      "22:55    Hidden layers:          (100, 100)\n",
      "22:55    Activation function:    tanh\n",
      "22:55    Batch size:             128\n",
      "22:55    Trainer:                amsgrad\n",
      "22:55    Epochs:                 50\n",
      "22:55    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "22:55    Validation split:       None\n",
      "22:55    Early stopping:         True\n",
      "22:55    Scale inputs:           True\n",
      "22:55    Regularization:         None\n",
      "22:55  Loading training data\n",
      "22:55  Found 500000 samples with 2 parameters and 23 observables\n",
      "22:55  Rescaling inputs\n",
      "22:55  Only using 23 of 23 observables\n",
      "22:55  Creating model for method sally\n",
      "22:55  Training model\n",
      "22:56    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "22:57    Epoch 10: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:58    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:59    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "22:59    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:00    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:01    Epoch 35: train loss 0.0000 (mse_score: 0.0000)\n",
      "23:02    Epoch 40: train loss 0.0000 (mse_score: 0.0000)\n",
      "23:02    Epoch 45: train loss 0.0000 (mse_score: 0.0000)\n",
      "23:03    Epoch 50: train loss 0.0000 (mse_score: 0.0000)\n",
      "23:03  Finished training\n",
      "23:03  Training estimator 4 / 5 in ensemble\n",
      "23:03  Starting training\n",
      "23:03    Method:                 sally\n",
      "23:03    Training data: x at data/samples_scoretest/x_train3.npy\n",
      "23:03                   t_xz (theta0) at  data/samples_scoretest/t_xz_train3.npy\n",
      "23:03    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "23:03    Method:                 sally\n",
      "23:03    Hidden layers:          (100, 100)\n",
      "23:03    Activation function:    tanh\n",
      "23:03    Batch size:             128\n",
      "23:03    Trainer:                amsgrad\n",
      "23:03    Epochs:                 50\n",
      "23:03    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:03    Validation split:       None\n",
      "23:03    Early stopping:         True\n",
      "23:03    Scale inputs:           True\n",
      "23:03    Regularization:         None\n",
      "23:03  Loading training data\n",
      "23:03  Found 500000 samples with 2 parameters and 23 observables\n",
      "23:03  Rescaling inputs\n",
      "23:03  Only using 23 of 23 observables\n",
      "23:03  Creating model for method sally\n",
      "23:03  Training model\n",
      "23:04    Epoch 5: train loss 0.0002 (mse_score: 0.0002)\n",
      "23:05    Epoch 10: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:06    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:06    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:07    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:08    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:09    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:09    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:10    Epoch 45: train loss 0.0000 (mse_score: 0.0000)\n",
      "23:11    Epoch 50: train loss 0.0000 (mse_score: 0.0000)\n",
      "23:11  Finished training\n",
      "23:11  Training estimator 5 / 5 in ensemble\n",
      "23:11  Starting training\n",
      "23:11    Method:                 sally\n",
      "23:11    Training data: x at data/samples_scoretest/x_train4.npy\n",
      "23:11                   t_xz (theta0) at  data/samples_scoretest/t_xz_train4.npy\n",
      "23:11    Features:               (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22)\n",
      "23:11    Method:                 sally\n",
      "23:11    Hidden layers:          (100, 100)\n",
      "23:11    Activation function:    tanh\n",
      "23:11    Batch size:             128\n",
      "23:11    Trainer:                amsgrad\n",
      "23:11    Epochs:                 50\n",
      "23:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "23:11    Validation split:       None\n",
      "23:11    Early stopping:         True\n",
      "23:11    Scale inputs:           True\n",
      "23:11    Regularization:         None\n",
      "23:11  Loading training data\n",
      "23:11  Found 500000 samples with 2 parameters and 23 observables\n",
      "23:11  Rescaling inputs\n",
      "23:11  Only using 23 of 23 observables\n",
      "23:11  Creating model for method sally\n",
      "23:11  Training model\n",
      "23:12    Epoch 5: train loss 0.0003 (mse_score: 0.0003)\n",
      "23:12    Epoch 10: train loss 0.0002 (mse_score: 0.0002)\n",
      "23:13    Epoch 15: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:14    Epoch 20: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:15    Epoch 25: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:15    Epoch 30: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:16    Epoch 35: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:17    Epoch 40: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:17    Epoch 45: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:18    Epoch 50: train loss 0.0001 (mse_score: 0.0001)\n",
      "23:18  Finished training\n"
     ]
    }
   ],
   "source": [
    "#NN Settings\n",
    "n_hidden = (100,100)\n",
    "n_epochs = 50\n",
    "batch_size = 128\n",
    "initial_lr=0.001\n",
    "final_lr=0.0001\n",
    "myfeatures=[(0,1,2,3,4,5,6,7,8),(0,1,2,3,4,5,6,7,8,9,10,11),(12,13,14,15,16,17,18,19,20,21,22),(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22)]\n",
    "myfeaturename=[\"mom\",\"enmom\",\"high\",\"all\"]\n",
    "\n",
    "#Run NN\n",
    "for ii in range(len(myfeatures)):\n",
    "    #Setup ensemble\n",
    "    ensemble = EnsembleForge(estimators=n_estimators)\n",
    "    #train ensemble\n",
    "    ensemble.train_all(\n",
    "        method='sally',\n",
    "        x_filename=[usesamplesdir+'x_train{}.npy'.format(i) for i in range(n_estimators)],\n",
    "        t_xz0_filename=[usesamplesdir+'t_xz_train{}.npy'.format(i) for i in range(n_estimators)],\n",
    "        n_epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=None,\n",
    "        n_hidden=n_hidden,\n",
    "        initial_lr=initial_lr,\n",
    "        final_lr=final_lr,\n",
    "        features=myfeatures[ii]\n",
    "    )\n",
    "    #save ensemble\n",
    "    ensemble.save(usemodelsdir+'sally_ensemble_{}'.format(myfeaturename[ii]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. FisherInfo for Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:18  Loading data from data/madminer_wgamma_xxx_observables_1M.h5\n",
      "23:18  Found 2 parameters:\n",
      "23:18     CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "23:18     CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "23:18  Found 6 benchmarks:\n",
      "23:18     sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00\n",
      "23:18     w: CWL2 = 20.00, CPWL2 = 0.00e+00\n",
      "23:18     morphing_basis_vector_2: CWL2 = -4.69e+01, CPWL2 = 27.54\n",
      "23:18     morphing_basis_vector_3: CWL2 = 21.00, CPWL2 = 46.69\n",
      "23:18     morphing_basis_vector_4: CWL2 = 32.29, CPWL2 = -4.64e+01\n",
      "23:18     morphing_basis_vector_5: CWL2 = -3.43e+01, CPWL2 = -3.61e+01\n",
      "23:18  Found 23 observables: px_l, px_v, px_a, py_l, py_v, py_a, pz_l, pz_v, pz_a, e_l, e_v, e_a, pt_l, pt_v, pt_a, eta_l, eta_v, eta_a, dphi_lv, dphi_la, dphi_va, m_lv, m_lva\n",
      "23:18  Found 1000000 events\n",
      "23:18  Found morphing setup with 6 components\n",
      "23:19  Evaluating rate Fisher information\n",
      "23:20  Found ensemble with 5 estimators and expectations None\n",
      "23:21  Uncertainty on Fisher information with single SALLY instance only reflects the covariance from the rate, not the kinematic part!\n",
      "23:22  Evaluating rate Fisher information\n",
      "23:23  Found ensemble with 5 estimators and expectations None\n",
      "23:23  Uncertainty on Fisher information with single SALLY instance only reflects the covariance from the rate, not the kinematic part!\n",
      "23:24  Evaluating rate Fisher information\n",
      "23:25  Found ensemble with 5 estimators and expectations None\n",
      "23:26  Uncertainty on Fisher information with single SALLY instance only reflects the covariance from the rate, not the kinematic part!\n",
      "23:27  Evaluating rate Fisher information\n",
      "23:28  Found ensemble with 5 estimators and expectations None\n",
      "23:28  Uncertainty on Fisher information with single SALLY instance only reflects the covariance from the rate, not the kinematic part!\n"
     ]
    }
   ],
   "source": [
    "#Setup MadFisher\n",
    "fisher = FisherInformation(useinputdata, debug=False)\n",
    "\n",
    "#Run MadFisher\n",
    "fi_det_mean_all, fi_det_cov_all = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.], luminosity=300*1000.,return_error=True,\n",
    "    model_file=usemodelsdir+'sally_ensemble_all',\n",
    "    unweighted_x_sample_file=usesamplesdir+'x_test.npy'\n",
    ")\n",
    "fi_det_mean_high, fi_det_cov_high = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.], luminosity=300*1000.,return_error=True,\n",
    "    model_file=usemodelsdir+'sally_ensemble_high',\n",
    "    unweighted_x_sample_file=usesamplesdir+'x_test.npy'\n",
    ")\n",
    "fi_det_mean_mom, fi_det_cov_mom = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.], luminosity=300*1000.,return_error=True,\n",
    "    model_file=usemodelsdir+'sally_ensemble_mom',\n",
    "    unweighted_x_sample_file=usesamplesdir+'x_test.npy'\n",
    ")\n",
    "fi_det_mean_enmom, fi_det_cov_enmom = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.], luminosity=300*1000.,return_error=True,\n",
    "    model_file=usemodelsdir+'sally_ensemble_enmom',\n",
    "    unweighted_x_sample_file=usesamplesdir+'x_test.npy'\n",
    ")\n",
    "\n",
    "#Run MadFisher Truth\n",
    "fi_pl_full,_ = fisher.calculate_fisher_information_full_truth(theta=[0.,0.],luminosity=300*1000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dbbcfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot FisherInfo\n",
    "fignn = plot_fisher_information_contours_2d(\n",
    "    [fi_det_mean_all,fi_det_mean_mom,fi_det_mean_enmom,fi_det_mean_high, fi_pl_full],\n",
    "    [fi_det_cov_all ,fi_det_cov_mom ,fi_det_cov_enmom ,fi_det_cov_high , None      ],\n",
    "    colors=[u'C0',u'C1',u'C2',u'C3',\"black\"],\n",
    "    linestyles=[\"solid\",\"solid\",\"solid\",\"solid\",\"dashed\"],\n",
    "    inline_labels=[\"all\",\"mom\",\"enmom\",\"high\",\"truth\"],\n",
    "    xrange=(-7.,7.),\n",
    "    yrange=(-4.,4.),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
