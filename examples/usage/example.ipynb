{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll demonstrate how to use MadMiner to generate train and test samples for the ML methods introduced in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020), both by Johann Brehmer, Gilles Louppe, Juan Pavez, and Kyle Cranmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you execute this notebook, make sure you have running installations of MadGraph, Pythia, and Delphes. Note that at least for now, the MG-Pythia interface and Delphes require custom patches (available upon request). In addition, MadMiner and [DelphesMiner](https://github.com/johannbrehmer/delphesminer) have to be in your PYTHONPATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from madminer.goldmine import GoldMine\n",
    "from madminer.tools.plots import plot_2d_morphing_basis\n",
    "\n",
    "from delphesprocessor.delphesprocessor import DelphesProcessor\n",
    "\n",
    "from madminer.refinery import combine_and_shuffle\n",
    "from madminer.refinery import Refinery\n",
    "from madminer.refinery import constant_benchmark_theta, multiple_benchmark_thetas\n",
    "from madminer.refinery import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "\n",
    "from forge.forge import Forge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter here the path to your MG5 root directory. This notebook assumes that you installed Delphes and Pythia through MG5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a `GoldMine` instance, the first important step is the definition of the parameter space. Each model parameter is characterized by a name as well as the LHA block and ID.\n",
    "\n",
    "If morphing is used, one also has to specify the maximal power with which the parameter contributes to the squared matrix element. For instance, a parameter that contributes only to one vertex, will typically have `morphing_max_power=2`, while a parameter that contributes to two vertices usually has `morphing_max_power=4`. Exceptions arise for instance when the interference effects between the SM and dimension-six operators are modelled, but the square of the dimension-six amplitude (subleading in 1/Lambda) is not taken into account, in which case `morphing_max_power=1`. Finally, the `parameter_range` argument defines the range of parameter values that are used for the automatic optimization of the morphing basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner = GoldMine()\n",
    "\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=2,\n",
    "    parameter_name='CWL2',\n",
    "    morphing_max_power=2,\n",
    "    parameter_range=(-10.,10.)\n",
    ")\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=5,\n",
    "    parameter_name='CPWL2',\n",
    "    morphing_max_power=2,\n",
    "    parameter_range=(-10.,10.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define benchmark points (evaluation points for |M|^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of all the points at which the weights (squared matrix elements) should be evaluated by MadGraph. We call these points \"benchmarks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Set benchmarks by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define benchmarks by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.add_benchmark(\n",
    "    {'CWL2':0., 'CPWL2':0.},\n",
    "    'sm'\n",
    ")\n",
    "miner.add_benchmark(\n",
    "    {'CWL2':1., 'CPWL2':0.},\n",
    "    'bsm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Benchmarks for morphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If morphing is used, the function `set_benchmarks_from_morphing` has to be called. With the option `keep_existing_benchmarks=True`, MadMiner will keep all the benchmark points defined beforehand and run a simple optimization algorithm to fix the remaining ones for the basis (which may be none). Otherwise, MadMiner will optimize the full basis and forget about all previously defined benchmark points. The argument `n_trials` determines the number of random candidate bases that the optimization algorithm goes through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.set_benchmarks_from_morphing(\n",
    "    keep_existing_benchmarks=True,\n",
    "    n_trials=1000,\n",
    "    max_overall_power=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the resulting morphing basis and the \"morphing error\", i.e. the sum of squared morphing weights as a function of the parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_2d_morphing_basis(\n",
    "    miner.morpher,\n",
    "    xlabel=r'$c_{W} / \\Lambda^2$ [TeV$^{-2}$]',\n",
    "    ylabel=r'$c_{\\tilde{W}} / \\Lambda^2$ [TeV$^{-2}$]',\n",
    "    xrange=(-10.,10),\n",
    "    yrange=(-10.,10.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save settings and run MadGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space, benchmark points, and morphing setup are saved in a HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.save('data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, MadMiner starts MadGraph and Pythia to generate events and calculate the weights. You have to provide paths to the process card, run card, param card (the entries corresponding to the parameters of interest will be automatically adapted), and an empty reweight card.\n",
    "\n",
    "The `sample_benchmark` option can be used to specify which benchmark should be used for sampling. If it is not used, MadMiner will automatically use the benchmark that was added first. Finally, if MadGraph is supposed to run in a different Python environment or requires other setup steps, you can use the `initial_command` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.run(\n",
    "    mg_directory=mg_dir,\n",
    "    proc_card_file='cards/proc_card.dat',\n",
    "    param_card_template_file='cards/param_card_template.dat',\n",
    "    reweight_card_template_file='cards/reweight_card_template.dat',\n",
    "    run_card_file='cards/run_card.dat',\n",
    "    pythia8_card_file='cards/pythia8_card.dat',\n",
    "    sample_benchmark='sm',\n",
    "    initial_command='source activate python2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run detector simulation and extract observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The detector simulation and calculation of observables is not part of MadMiner. The reason is that different users might have very different requirements here: while a phenomenologist might be content with the fast detector simulation from Delphes, an experimental analysis might require the full simulation through Geant4.\n",
    "\n",
    "We provide the DelphesMiner package, which wraps around Delphes and allows for the fast extraction of observables into the HDF5 file.\n",
    "\n",
    "Any user is free to replace the DelphesMiner step with a tool of their choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DelphesProcessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the DelphesProcessor object, one can add a number of HepMC event samples..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.add_hepmc_sample('MG_process/Events/run_01/tag_1_pythia8_events.hepmc.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and have it run Delphes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.run_delphes(delphes_directory=mg_dir + '/Delphes',\n",
    "               delphes_card='cards/delphes_card.dat',\n",
    "               initial_command='source activate python2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of observables through a name and a python expression. For the latter, you can use the objects `j[i]`, `e[i]`, `mu[i]`, `a[i]`, `met`, where the indices `i` refer to a ordering by the transverse momentum. All of these objects are scikit-hep [LorentzVectors](http://scikit-hep.org/api/math.html#vector-classes), see the link for a documentation of their properties.\n",
    "\n",
    "There is an optional keyword `required`. If `required=True`, we will only keep events where the observable can be parsed, i.e. all involved particles have been detected. If `required=False`, un-parseable observables will be filled with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.add_observable('pt_j1', 'j[0].pt', required=True)\n",
    "dm.add_observable('pt_j2', 'j[1].pt', required=True)\n",
    "dm.add_observable('delta_phi_jj', 'abs(j[0].phi() - j[1].phi())', required=True)\n",
    "dm.add_observable('delta_eta_jj', 'abs(j[0].eta - j[1].eta)', required=True)\n",
    "dm.add_observable('m_jj', '(j[0] + j[1]).m', required=True)\n",
    "dm.add_observable('n_jets', 'len(j)', required=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `analyse_delphes_samples` extracts all these observables from the Delphes ROOT file(s) generated before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.analyse_delphes_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the observables and the weights are then saved in the HDF5 file. It is possible to overwrite the same file, or to leave the original file intact and save all the data into a new file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.save('data/madminer_example_with_data.h5', 'data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to check some distributions at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "for weights in dm.weights:\n",
    "    plt.hist(dm.observations['pt_j1'], range=(0.,800.), bins=20, histtype='step', weights=weights)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine and shuffle different event samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce disk usage, you can generate several small event samples with the steps given above, and combine them now. Note that (for now) it is essential that all of them are generated with the same setup, including the same benchmark points / morphing basis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_and_shuffle(\n",
    "    ['data/madminer_example_with_data.h5'],\n",
    "    'data/madminer_example_shuffled.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last important MadMiner class is the `Smithy`. From all the data we have in the HDF5 file now, it extracts unweighted samples including the augmented data (\"gold\") that is needed as training and evaluation data for the Machine Learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refinery = Refinery('data/madminer_example_shuffled.h5', debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Refinery` class defines four different high-level functions to generate train or test samples:\n",
    "- `extract_samples_train_plain()`, which only saves observations x, for instance for histograms or ABC;\n",
    "- `extract_samples_train_local()` for methods like SALLY and SALLINO;\n",
    "- `extract_samples_train_ratio()` for techniques like CARL, ROLR, CASCAL, and RASCAL; and\n",
    "- `extract_samples_test()` for the evaluation of any method.\n",
    "\n",
    "For the arguments `theta`, `theta0`, or `theta1`, you can use the helper functions `constant_benchmark_theta()`, `multiple_benchmark_thetas()`, `constant_morphing_theta()`, `multiple_morphing_thetas()`, and `random_morphing_thetas()`, all defined in the `smithy` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta, t_xz = refinery.extract_samples_train_local(\n",
    "    theta=constant_morphing_theta(np.array([1.,0.])),\n",
    "    n_samples=1000,\n",
    "    folder='./data/samples',\n",
    "    filename='train_sally'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta0, theta1, y, r_xz, t_xz = refinery.extract_samples_train_ratio(\n",
    "    theta0=random_morphing_thetas(None, [('gaussian', 0., 5.), ('flat', -10., 10.)]),\n",
    "    theta1=constant_benchmark_theta('sm'),\n",
    "    n_samples=10000,\n",
    "    folder='./data/samples',\n",
    "    filename='train_rascal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, theta = refinery.extract_samples_test(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=10000,\n",
    "    folder='./data/samples',\n",
    "    filename='test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some distributions and correlations in this test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "labels = [r'$p_{T,j1}$ [GeV]', r'$p_{T,j2}$ [GeV]', r'$\\Delta \\phi_{jj}$', r'$\\Delta \\eta_{jj}$', r'$m_{jj}$', r'$n_{j}$']\n",
    "ranges = [(0., 300.), (0., 200.), (0.,6.2), (0.,8.), (0.,1500.), (1.,6.5)]\n",
    "\n",
    "_ = corner.corner(x, color='C0', labels=labels, range=ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging, you can also access the full list of observables and benchmark weights (same units as in the LHE file) in the HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x, all_weights = refinery.extract_raw_data(theta=[0.0,0.1])\n",
    "\n",
    "print(all_x)\n",
    "print(all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train likelihood ratio estimator (e.g. RASCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to train the likelihood ratio estimators. The central object for this is the `Forge` class. It defines functions to train the ratio estimators, to evaluate arbitrary samples, as well as to save and load the trained models.\n",
    "\n",
    "Here we will use the RASCAL method described in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). Other soon-to-be implemented methods include CARL, CASCAL, and ROLR described in the same publications, as well as ALICE and ALICES which are introduced in a paper that will soon hit the arXiv. SALLY and SALLINO will follow at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:10  \n",
      "19:10  ------------------------------------------------------------\n",
      "19:10  |                                                          |\n",
      "19:10  |  Forge                                                   |\n",
      "19:10  |                                                          |\n",
      "19:10  |  Version from July 31, 2018                              |\n",
      "19:10  |                                                          |\n",
      "19:10  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "19:10  |                                                          |\n",
      "19:10  ------------------------------------------------------------\n",
      "19:10  \n",
      "19:10  Starting training\n",
      "19:10    Method:                 rascal\n",
      "19:10    Training data: theta at data/samples/theta0_train_rascal.npy\n",
      "19:10                   x at     data/samples/x_train_rascal.npy\n",
      "19:10                   y at     data/samples/y_train_rascal.npy\n",
      "19:10                   r_xz at  data/samples/r_xz_train_rascal.npy\n",
      "19:10                   t_xz at  data/samples/t_xz_train_rascal.npy\n",
      "19:10    Method:                 rascal\n",
      "19:10    Hidden layers:          (100, 100, 100)\n",
      "19:10    Activation function:    tanh\n",
      "19:10    alpha:                  100.0\n",
      "19:10    Batch size:             64\n",
      "19:10    Epochs:                 10\n",
      "19:10    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "19:10    Early stopping:         True\n",
      "19:10  Loading training data\n",
      "19:10  Found 10000 samples with 2 parameters and 6 observables\n",
      "19:10  Creating model for method rascal\n",
      "19:10  Training model\n",
      "19:10  Starting training\n",
      "19:10  Transforming ParameterizedRatioEstimator to (device(type='cpu'), torch.float32)\n",
      "19:10    Epoch 1: train loss 15.57 ([1.54601747e+01 1.11345109e-03]), validation loss 0.45 ([0.35565211 0.00096491]) (*)\n",
      "19:11    Epoch 2: train loss 15.60 ([1.5485385e+01 1.1127466e-03]), validation loss 0.45 ([0.35073347 0.00096192]) (*)\n",
      "19:11    Epoch 3: train loss 15.58 ([1.54635708e+01 1.12831404e-03]), validation loss 0.45 ([0.35482778 0.00096535])\n",
      "19:11    Epoch 4: train loss 15.59 ([1.54779916e+01 1.12170552e-03]), validation loss 0.45 ([0.35077596 0.00097227])\n",
      "19:11    Epoch 5: train loss 15.55 ([1.54413809e+01 1.11611800e-03]), validation loss 0.46 ([0.35770388 0.00098042])\n",
      "19:11    Epoch 6: train loss 15.53 ([1.54165401e+01 1.13093279e-03]), validation loss 0.45 ([0.35520588 0.00099421])\n",
      "19:11    Epoch 7: train loss 15.53 ([1.54163735e+01 1.14481753e-03]), validation loss 0.45 ([0.35293108 0.00100194])\n",
      "19:11    Epoch 8: train loss 15.53 ([1.54113803e+01 1.14018768e-03]), validation loss 0.49 ([0.39200066 0.00098194])\n",
      "19:12    Epoch 9: train loss 15.46 ([1.53389494e+01 1.17543894e-03]), validation loss 0.46 ([0.35479558 0.00101553])\n",
      "19:12    Epoch 10: train loss 15.46 ([1.53444733e+01 1.16702513e-03]), validation loss 0.47 ([0.35807004 0.00108308])\n",
      "19:12  Early stopping after epoch 2, with loss 0.45 compared to final loss 0.47\n",
      "19:12  Finished training\n",
      "19:12  Saving settings to models/rascal.pt_settings.json\n",
      "19:12  Saving state dictionary to models/rascal.pt_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "forge = Forge(debug=True)\n",
    "\n",
    "forge.train(\n",
    "    method='rascal',\n",
    "    theta_filename='data/samples/theta0_train_rascal.npy',\n",
    "    x_filename='data/samples/x_train_rascal.npy',\n",
    "    y_filename='data/samples/y_train_rascal.npy',\n",
    "    r_xz_filename='data/samples/r_xz_train_rascal.npy',\n",
    "    t_xz_filename='data/samples/t_xz_train_rascal.npy',\n",
    "    alpha=100.,\n",
    "    n_epochs=10,\n",
    "    validation_split=0.5\n",
    ")\n",
    "\n",
    "forge.save('models/rascal.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
