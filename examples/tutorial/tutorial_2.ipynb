{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner tutorial 2: SALLY, Fisher information, and ensemble methods\n",
    "\n",
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first tutorial you saw how to set up a process with MadMiner, generate events and turn them into training samples, and construct likelihood ratio estimators from them. Here we introduce some alternative steps which will lead us to an estimator for the score at a reference point (SALLY) and the expected Fisher information. Along the way, we'll introduce some powerful ensemble methods.\n",
    "\n",
    "If you're not familiar with SALLY, please have a look at [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013)\n",
    "or, for more details, [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). For the Fisher information part, see the original publication [\"Better Higgs Measurements Through Information Geometry\"](https://arxiv.org/abs/1612.05261) or a more detailed, pedagogical introduction in Chapter 4 of [\"New Ideas for Effective Higgs Measurements\"](https://inspirehep.net/record/1624219)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've run the first tutorial before executing this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from madminer.sampling import SampleAugmenter, constant_benchmark_theta\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "from madminer.fisherinformation import FisherInformation\n",
    "from madminer.plotting import plot_fisher_information_contours_2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter here the path to your MG5 root directory. This notebook assumes that you installed Delphes and Pythia through MG5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MadMiner uses the Python `logging` module to provide additional information and debugging output. You can choose how much of this output you want to see by switching the level in the following lines to `logging.DEBUG` or `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 5.: see tutorial 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have run steps 1 through 5 of the first tutorial, and thus have a MadMiner file with observables and event weights ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the next step is handled by the MadMiner class `SampleAugmenter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:13 madminer.sampling    INFO    Loading data from data/madminer_example_shuffled.h5\n",
      "16:13 madminer.sampling    INFO    Found 2 parameters\n",
      "16:13 madminer.sampling    INFO    Did not find nuisance parameters\n",
      "16:13 madminer.sampling    INFO    Found 6 benchmarks, of which 6 physical\n",
      "16:13 madminer.sampling    INFO    Found 2 observables\n",
      "16:13 madminer.sampling    INFO    Found 1018 events\n",
      "16:13 madminer.sampling    INFO    Found morphing setup with 6 components\n"
     ]
    }
   ],
   "source": [
    "sa = SampleAugmenter('data/madminer_example_shuffled.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant `SampleAugmenter` function for local score estimators is `extract_samples_train_local()`. As before, for the argument `theta` you can use the helper functions `constant_benchmark_theta()`, `multiple_benchmark_thetas()`, `constant_morphing_theta()`, `multiple_morphing_thetas()`, and `random_morphing_thetas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:13 madminer.sampling    INFO    Extracting training sample for local score regression. Sampling and score evaluation according to ('benchmark', 'sm')\n",
      "16:13 madminer.sampling    WARNING Warning: large statistical uncertainty on the total cross section for theta = [0. 0.]: (0.01027308101999995 +/- 0.001511271117152794) pb\n",
      "16:13 madminer.sampling    INFO    Effective number of samples: 44.0585179822994\n"
     ]
    }
   ],
   "source": [
    "x, theta, t_xz = sa.extract_samples_train_local(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='train0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. Train one network to estimate score and Fisher information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build a neural network. Only this time, instead of the likelihood ratio itself, we will estimate the gradient of the log likelihood with respect to the theory parameters -- the score. To be precise, the output of the neural network is an estimate of the score at some reference parameter point, for instance the Standard Model. A neural network that estimates this \"local\" score can be used to calculate the Fisher information at that point. The estimated score can also be used as a machine learning version of Optimal Observables, and likelihoods can be estimated based on density estimation in the estimated score space. This method for likelihood ratio estimation is called SALLY, and there is a closely related version called SALLINO. Both are explained in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020).\n",
    "\n",
    "Again, the central object for this is the `madminer.ml.MLForge` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:13 madminer.ml          INFO    Starting training\n",
      "16:13 madminer.ml          INFO      Method:                 sally\n",
      "16:13 madminer.ml          INFO      Training data: x at data/samples/x_train0.npy\n",
      "16:13 madminer.ml          INFO                     t_xz (theta0) at  data/samples/t_xz_train0.npy\n",
      "16:13 madminer.ml          INFO      Features:               all\n",
      "16:13 madminer.ml          INFO      Method:                 sally\n",
      "16:13 madminer.ml          INFO      Hidden layers:          (100, 100)\n",
      "16:13 madminer.ml          INFO      Activation function:    tanh\n",
      "16:13 madminer.ml          INFO      Batch size:             256\n",
      "16:13 madminer.ml          INFO      Trainer:                amsgrad\n",
      "16:13 madminer.ml          INFO      Epochs:                 50\n",
      "16:13 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "16:13 madminer.ml          INFO      Validation split:       None\n",
      "16:13 madminer.ml          INFO      Early stopping:         True\n",
      "16:13 madminer.ml          INFO      Scale inputs:           True\n",
      "16:13 madminer.ml          INFO      Shuffle labels          False\n",
      "16:13 madminer.ml          INFO      Regularization:         None\n",
      "16:13 madminer.ml          INFO      Samples:                all\n",
      "16:13 madminer.ml          INFO    Loading training data\n",
      "16:13 madminer.ml          INFO    Found 100000 samples with 2 parameters and 2 observables\n",
      "16:13 madminer.ml          INFO    Rescaling inputs\n",
      "16:13 madminer.ml          INFO    Creating model for method sally\n",
      "16:13 madminer.ml          INFO    Training model\n",
      "16:13 madminer.utils.ml.sc INFO      Epoch 05: train loss 0.0784 (mse_score: 0.0784)\n",
      "16:13 madminer.utils.ml.sc INFO      Epoch 10: train loss 0.0477 (mse_score: 0.0477)\n",
      "16:13 madminer.utils.ml.sc INFO      Epoch 15: train loss 0.0322 (mse_score: 0.0322)\n",
      "16:13 madminer.utils.ml.sc INFO      Epoch 20: train loss 0.0249 (mse_score: 0.0249)\n",
      "16:13 madminer.utils.ml.sc INFO      Epoch 25: train loss 0.0209 (mse_score: 0.0209)\n",
      "16:13 madminer.utils.ml.sc INFO      Epoch 30: train loss 0.0187 (mse_score: 0.0187)\n",
      "16:14 madminer.utils.ml.sc INFO      Epoch 35: train loss 0.0173 (mse_score: 0.0173)\n",
      "16:14 madminer.utils.ml.sc INFO      Epoch 40: train loss 0.0164 (mse_score: 0.0164)\n",
      "16:14 madminer.utils.ml.sc INFO      Epoch 45: train loss 0.0159 (mse_score: 0.0159)\n",
      "16:14 madminer.utils.ml.sc INFO      Epoch 50: train loss 0.0153 (mse_score: 0.0153)\n",
      "16:14 madminer.utils.ml.sc INFO    Finished training\n"
     ]
    }
   ],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename='data/samples/x_train0.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train0.npy',\n",
    "    n_epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_split=None\n",
    ")\n",
    "\n",
    "forge.save('models/sally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the SM score on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "evaluate() got an unexpected keyword argument 'x_filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-db3d208c61a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m t_hat = forge.evaluate(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mx_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/samples/x_test.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: evaluate() got an unexpected keyword argument 'x_filename'"
     ]
    }
   ],
   "source": [
    "forge.load('models/sally')\n",
    "\n",
    "t_hat = forge.evaluate(\n",
    "    x_filename='data/samples/x_test.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the estimated score and how it is related to the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/samples/x_test.npy')\n",
    "skip=10\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "\n",
    "    sc = plt.scatter(x[::skip,0], x[::skip,1], c=t_hat[::skip,i], s=10., cmap='viridis', vmin=-0.05, vmax=0.02)\n",
    "    cbar = plt.colorbar(sc)\n",
    "\n",
    "    cbar.set_label(r'$\\hat{t}_' + str(i) + r'(x | \\theta_{ref})$')\n",
    "    plt.xlabel(r'$p_{T,j1}$ [GeV]')\n",
    "    plt.ylabel(r'$\\Delta \\phi_{jj}$')\n",
    "    plt.xlim(10.,400.)\n",
    "    plt.ylim(0.,6.2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fisher information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can use SALLY estimators to estimate the expected Fisher information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_information, _ = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally',\n",
    "    unweighted_x_sample_file='data/samples/x_test.npy',\n",
    "    luminosity=300000.\n",
    ")\n",
    "\n",
    "print('Kinematic Fisher information after 300 ifb:\\n{}'.format(fisher_information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Fisher information with contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information],\n",
    "    xrange=(-2.,2.),\n",
    "    yrange=(-2.,2.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a single neural network to estimate the likelihood ratio, score, or Fisher information, we can use an ensemble of such estimators. That provides us with a more reliable mean prediction as well as a measure of the uncertainty. The class `madminer.ml.EnsembleForge` automates this process. Currently, it only supports SALLY estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleForge(estimators=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EnsembleForge` object has very similar functions as `MLForge`. In particular, we can train all estimators simultaneously with `train_all()` and save the ensemble to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x_filename='data/samples/x_train0.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train0.npy',\n",
    "    n_epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_split=None\n",
    ")\n",
    "\n",
    "ensemble.save('models/sally_ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the ensemble similarly to the individual networks. Let's stick to the estimation of the Fisher information. There are two different ways to take the ensemble average:\n",
    "\n",
    "- `mode='information'`: We can calculate the Fisher information for each estimator in the ensemble, and then take the mean and the covariance over the ensemble. This has the advantage that it provides a direct measure of the uncertainty of the prediction.\n",
    "- `mode='score'`: We can calculate the score for each event and estimator, take the ensemble mean for the score of each event, and then calculate the Fisher information based on the mean scores. This is expected to be more precise (since the score estimates will be more precise, and the nonlinearity in the Fisher info calculation amplifies any error in the score estimation). But calculating the covariance in this approach is computationally not feasible, so there will be no error bands.\n",
    "\n",
    "By default, MadMiner uses the 'score' mode. Here we will use the 'information' mode just to show the nice uncertainty bands we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5', debug=False)\n",
    "\n",
    "fisher_information_mean, fisher_information_covariance = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally_ensemble',\n",
    "    unweighted_x_sample_file='data/samples/x_test.npy',\n",
    "    luminosity=300000.,\n",
    "    mode='information'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance can be propagated to the Fisher distance contour plot easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information_mean],\n",
    "    [fisher_information_covariance],\n",
    "    xrange=(-1.,1.),\n",
    "    yrange=(-1.,1.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of our second tutorial. If you have questions, please have a look at the papers, the module documentation, or drop us an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (higgs_inference)",
   "language": "python",
   "name": "higgs_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
