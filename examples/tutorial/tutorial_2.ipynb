{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner tutorial 2: SALLY, Fisher information, and ensemble methods\n",
    "\n",
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first tutorial you saw how to set up a process with MadMiner, generate events and turn them into training samples, and construct likelihood ratio estimators from them. Here we introduce some alternative steps which will lead us to an estimator for the score at a reference point (SALLY) and the expected Fisher information. Along the way, we'll introduce some powerful ensemble methods.\n",
    "\n",
    "If you're not familiar with SALLY, please have a look at [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013)\n",
    "or, for more details, [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). For the information geometry part, see the original publication [\"Better Higgs Measurements Through Information Geometry\"](https://arxiv.org/abs/1612.05261) or a more detailed, pedagogical introduction in Chapter 4 of [\"New Ideas for Effective Higgs Measurements\"](https://inspirehep.net/record/1624219)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you've run the first tutorial before executing this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from madminer.plotting import plot_2d_morphing_basis\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import constant_benchmark_theta, multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "from madminer.plotting import plot_fisher_information_contours_2d\n",
    "from madminer.fisherinformation import FisherInformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter here the path to your MG5 root directory. This notebook assumes that you installed Delphes and Pythia through MG5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 5.: see tutorial 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that you have run steps 1 through 5 of the first tutorial, and thus have a MadMiner file with observables and event weights ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the next step is handled by the MadMiner class `SampleAugmenter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SampleAugmenter('data/madminer_example_shuffled.h5', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant `SampleAugmenter` function for local score estimators is `extract_samples_train_local()`. As before, for the argument `theta` you can use the helper functions `constant_benchmark_theta()`, `multiple_benchmark_thetas()`, `constant_morphing_theta()`, `multiple_morphing_thetas()`, and `random_morphing_thetas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta, t_xz = sa.extract_samples_train_local(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=1000,\n",
    "    folder='./data/samples',\n",
    "    filename='train0'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7a. Train a neural network to estimate the score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build a neural network. Only this time, instead of the likelihood ratio itself, we will estimate the gradient of the log likelihood with respect to the theory parameters -- the score. To be precise, the output of the neural network is an estimate of the score at some reference parameter point, for instance the Standard Model. A neural network that estimates this \"local\" score can be used to calculate the Fisher information at that point. The estimated score can also be used as a machine learning version of Optimal Observables, and likelihoods can be estimated based on density estimation in the estimated score space. This method for likelihood ratio estimation is called SALLY, and there is a closely related version called SALLINO. Both are explained in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020).\n",
    "\n",
    "Again, the central object for this is the `madminer.ml.MLForge` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.train(\n",
    "    method='sally',\n",
    "    x_filename='data/samples/x_train0.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train0.npy',\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None\n",
    ")\n",
    "\n",
    "forge.save('models/sally')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the SM score on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.load('models/sally')\n",
    "\n",
    "t_hat = forge.evaluate(\n",
    "    x_filename='data/samples/x_test.npy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the estimated score and how it is related to the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('data/samples/x_test.npy')\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    ax = plt.subplot(1,2,i+1)\n",
    "\n",
    "    sc = plt.scatter(x[:,0], x[:,1], c=t_hat[:,0], s=10., cmap='viridis', vmin=-0.05, vmax=0.02)\n",
    "    cbar = plt.colorbar(sc)\n",
    "\n",
    "    cbar.set_label(r'$\\hat{t}_' + str(i) + r'(x | \\theta_{ref})$')\n",
    "    plt.xlabel(r'$p_{T,j1}$ [GeV]')\n",
    "    plt.ylabel(r'$\\Delta \\phi_{jj}$')\n",
    "    plt.xlim(10.,400.)\n",
    "    plt.ylim(0.,6.2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we can use SALLY estimators to estimate the expected Fisher information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5', debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_information = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally',\n",
    "    unweighted_x_sample_file='data/samples/x_test.npy',\n",
    "    luminosity=300000.\n",
    ")\n",
    "\n",
    "print('Kinematic Fisher information after 300 ifb:\\n{}'.format(fisher_information))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the Fisher information with contours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information],\n",
    "    xrange=(-2.,2.),\n",
    "    yrange=(-2.,2.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7b. Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a single neural network to estimate the likelihood ratio, score, or Fisher information, we can use an ensemble of such estimators. That provides us with a more reliable mean prediction as well as a measure of the uncertainty. The class `madminer.ml.EnsembleForge` automates this process. Currently, it only supports SALLY estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleForge(estimators=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `EnsembleForge` object has very similar functions as `MLForge`. In particular, we can train all estimators simultaneously with `train_all()` and save the ensemble to files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x_filename='data/samples/x_train0.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train0.npy',\n",
    "    n_epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_split=None\n",
    ")\n",
    "\n",
    "ensemble.save('models/sally_ensemble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any evaluation of this ensemble will provide us with mean and variance of the predictions. Let's try that for the Fisher information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher = FisherInformation('data/madminer_example_shuffled.h5', debug=False)\n",
    "\n",
    "fisher_information_mean, fisher_information_covariance = fisher.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/sally_ensemble',\n",
    "    unweighted_x_sample_file='data/samples/x_test.npy',\n",
    "    luminosity=300000.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance can be propagated to the Fisher distance contour plot easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fisher_information_mean],\n",
    "    [fisher_information_covariance],\n",
    "    xrange=(-2.,2.),\n",
    "    yrange=(-2.,2.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confused? If you have questions, please have a look at the papers, the module documentation, or drop us an email!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
