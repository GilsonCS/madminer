{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MadMiner tutorial 1: From cards to likelihood ratios\n",
    "\n",
    "Johann Brehmer, Felix Kling, Kyle Cranmer 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll introduce the basic MadMiner workflow. We'll show you how to use MadMiner to generate events, extract training data, and train neural networks to estimate likelihood ratios.\n",
    "\n",
    "This tutorial does not try to explain the inference methods. To understand what MadMiner is doing, please have a look at some papers first. In\n",
    "[\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013)\n",
    "we explain the basic idea of most of the methods presented here, while [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020) is an extensive 65-page handbook going through the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you execute this notebook, make sure you have running installations of MadGraph, Pythia, and Delphes. Note that at least for now, the MG-Pythia interface and Delphes require custom patches (available upon request). In addition, MadMiner has to be in your PYTHONPATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from madminer.core import MadMiner\n",
    "from madminer.plotting import plot_2d_morphing_basis\n",
    "from madminer.delphes import DelphesProcessor\n",
    "from madminer.sampling import combine_and_shuffle\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import constant_benchmark_theta, multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please enter here the path to your MG5 root directory. This notebook assumes that you installed Delphes and Pythia through MG5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg_dir = '/Users/johannbrehmer/work/projects/madminer/MG5_aMC_v2_6_2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a `GoldMine` instance, the first important step is the definition of the parameter space. Each model parameter is characterized by a name as well as the LHA block and ID.\n",
    "\n",
    "If morphing is used, one also has to specify the maximal power with which the parameter contributes to the squared matrix element. For instance, a parameter that contributes only to one vertex, will typically have `morphing_max_power=2`, while a parameter that contributes to two vertices usually has `morphing_max_power=4`. Exceptions arise for instance when the interference effects between the SM and dimension-six operators are modelled, but the square of the dimension-six amplitude (subleading in 1/Lambda) is not taken into account, in which case `morphing_max_power=1`. The `parameter_range` argument defines the range of parameter values that are used for the automatic optimization of the morphing basis.\n",
    "\n",
    "Finally, the parameter values theta used internally by MadMiner and the parameter values written to the param_card (or reweight_card) given to MadGraph do not have to be exactly the same. With the option `parm_card_transform`, the user can supply a one-parameter function that maps a parameter value theta to the value given to MadGraph. This string is a python expression, in which `theta` is parsed as the parameter value. For instance, if the internal parameters are in the range (0, 1), but should be linearly scaled to (0, 100) in the param_card, one would have to use `param_card_transform=\"100*theta\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34  \n",
      "15:34  ------------------------------------------------------------\n",
      "15:34  |                                                          |\n",
      "15:34  |  MadMiner v2018.10.26                                    |\n",
      "15:34  |                                                          |\n",
      "15:34  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "15:34  |                                                          |\n",
      "15:34  ------------------------------------------------------------\n",
      "15:34  \n",
      "15:34  Added parameter CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-10.0, 10.0))\n",
      "15:34  Added parameter CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-10.0, 10.0))\n"
     ]
    }
   ],
   "source": [
    "miner = MadMiner(debug=False)\n",
    "\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=2,\n",
    "    parameter_name='CWL2',\n",
    "    morphing_max_power=2,\n",
    "    param_card_transform=\"16.52*theta\",\n",
    "    parameter_range=(-10.,10.)\n",
    ")\n",
    "miner.add_parameter(\n",
    "    lha_block='dim6',\n",
    "    lha_id=5,\n",
    "    parameter_name='CPWL2',\n",
    "    morphing_max_power=2,\n",
    "    param_card_transform=\"16.52*theta\",\n",
    "    parameter_range=(-10.,10.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define benchmark points (evaluation points for |M|^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of all the points at which the weights (squared matrix elements) should be evaluated by MadGraph. We call these points \"benchmarks\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Set benchmarks by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can define benchmarks by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34  Added benchmark sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00)\n",
      "15:34  Added benchmark w: CWL2 = 10.00, CPWL2 = 0.00e+00)\n"
     ]
    }
   ],
   "source": [
    "miner.add_benchmark(\n",
    "    {'CWL2':0., 'CPWL2':0.},\n",
    "    'sm'\n",
    ")\n",
    "miner.add_benchmark(\n",
    "    {'CWL2':10., 'CPWL2':0.},\n",
    "    'w'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Benchmarks for morphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If morphing is used, the function `set_benchmarks_from_morphing` has to be called. With the option `keep_existing_benchmarks=True`, MadMiner will keep all the benchmark points defined beforehand and run a simple optimization algorithm to fix the remaining ones for the basis (which may be none). Otherwise, MadMiner will optimize the full basis and forget about all previously defined benchmark points. The argument `n_trials` determines the number of random candidate bases that the optimization algorithm goes through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34  Optimizing basis for morphing\n",
      "15:34  Added benchmark sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00)\n",
      "15:34  Added benchmark w: CWL2 = 10.00, CPWL2 = 0.00e+00)\n",
      "15:34  Added benchmark morphing_basis_vector_2: CWL2 = 7.63, CPWL2 = -8.69e+00)\n",
      "15:34  Added benchmark morphing_basis_vector_3: CWL2 = -8.59e+00, CPWL2 = -8.65e+00)\n",
      "15:34  Added benchmark morphing_basis_vector_4: CWL2 = -5.88e+00, CPWL2 = 5.33)\n",
      "15:34  Added benchmark morphing_basis_vector_5: CWL2 = 2.84, CPWL2 = 8.99)\n"
     ]
    }
   ],
   "source": [
    "miner.set_benchmarks_from_morphing(\n",
    "    keep_existing_benchmarks=True,\n",
    "    n_trials=1000,\n",
    "    max_overall_power=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the resulting morphing basis and the \"morphing error\", i.e. the sum of squared morphing weights as a function of the parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_2d_morphing_basis(\n",
    "    miner.morpher,\n",
    "    xlabel=r'$c_{W} v^2 / \\Lambda^2$',\n",
    "    ylabel=r'$c_{\\tilde{W}} v^2 / \\Lambda^2$',\n",
    "    xrange=(-10.,10.),\n",
    "    yrange=(-10.,10.)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save settings and run MadGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter space, benchmark points, and morphing setup are saved in a HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34  Saving setup (including morphing) to data/madminer_example.h5\n"
     ]
    }
   ],
   "source": [
    "miner.save('data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can now be loaded again with `miner.load(filename)`.\n",
    "\n",
    "In a next step, MadMiner starts MadGraph and Pythia to generate events and calculate the weights. You have to provide paths to the process card, run card, param card (the entries corresponding to the parameters of interest will be automatically adapted), and an empty reweight card. Log files in the `log_directory` folder collect the MadGraph output and are important for debugging.\n",
    "\n",
    "The `sample_benchmark` option can be used to specify which benchmark should be used for sampling. If it is not used, MadMiner will automatically use the benchmark that was added first.\n",
    "\n",
    "Finally, if MadGraph is supposed to run in a different Python environment or requires other setup steps, you can use the `initial_command` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:34  Generating MadGraph process folder from cards/proc_card_signal.dat at ./mg_processes/signal\n",
      "15:34  Run 0\n",
      "15:34    Sampling from benchmark: sm\n",
      "15:34    Original run card:       cards/run_card_signal.dat\n",
      "15:34    Original Pythia8 card:   cards/run_card_signal.dat\n",
      "15:34    Copied run card:         ./mg_processes/signal/madminer/cards/run_card_0.dat\n",
      "15:34    Copied Pythia8 card:     ./mg_processes/signal/madminer/cards/pythia8_card_0.dat\n",
      "15:34    Param card:              ./mg_processes/signal/madminer/cards/param_card_0.dat\n",
      "15:34    Reweight card:           ./mg_processes/signal/madminer/cards/reweight_card_0.dat\n",
      "15:34    Log file:                logs/signal/run_0.log\n",
      "15:34  Creating param and reweight cards in ./mg_processes/signal/madminer/cards/param_card_0.dat, ./mg_processes/signal/madminer/cards/reweight_card_0.dat\n",
      "15:34  Starting MadGraph and Pythia in ./mg_processes/signal\n"
     ]
    }
   ],
   "source": [
    "miner.run(\n",
    "    sample_benchmark='sm',\n",
    "    mg_directory=mg_dir,\n",
    "    mg_process_directory='./mg_processes/signal',\n",
    "    proc_card_file='cards/proc_card_signal.dat',\n",
    "    param_card_template_file='cards/param_card_template.dat',\n",
    "    reweight_card_template_file='cards/reweight_card_template.dat',\n",
    "    run_card_file='cards/run_card_signal.dat',\n",
    "    pythia8_card_file='cards/pythia8_card.dat',\n",
    "    log_directory='logs/signal',\n",
    "    initial_command='source activate python2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to start multiple processes based on the same `MadMiner` instance. This can be used to combine samples sampled according to different benchmarks, and to add reducible backgrounds. \n",
    "\n",
    "For the latter, a useful option is the `is_background` switch, which should be used for processes that do *not* depend on the parameters theta. `is_background=True` will disable the reweighting and re-use the same weights for all cross sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:05  Generating MadGraph process folder from cards/proc_card_background.dat at ./mg_processes/background\n",
      "16:06  Run 0\n",
      "16:06    Sampling from benchmark: sm\n",
      "16:06    Original run card:       cards/run_card_background.dat\n",
      "16:06    Original Pythia8 card:   cards/run_card_background.dat\n",
      "16:06    Copied run card:         ./mg_processes/background/madminer/cards/run_card_0.dat\n",
      "16:06    Copied Pythia8 card:     ./mg_processes/background/madminer/cards/pythia8_card_0.dat\n",
      "16:06    Param card:              ./mg_processes/background/madminer/cards/param_card_0.dat\n",
      "16:06    Reweight card:           ./mg_processes/background/madminer/cards/reweight_card_0.dat\n",
      "16:06    Log file:                logs/background/run_0.log\n",
      "16:06  Creating param and reweight cards in ./mg_processes/background/madminer/cards/param_card_0.dat, ./mg_processes/background/madminer/cards/reweight_card_0.dat\n",
      "16:06  Starting MadGraph and Pythia in ./mg_processes/background\n"
     ]
    }
   ],
   "source": [
    "miner.run(\n",
    "    is_background=True,\n",
    "    sample_benchmark='sm',\n",
    "    mg_directory=mg_dir,\n",
    "    mg_process_directory='./mg_processes/background',\n",
    "    proc_card_file='cards/proc_card_background.dat',\n",
    "    param_card_template_file='cards/param_card_template.dat',\n",
    "    reweight_card_template_file='cards/reweight_card_template.dat',\n",
    "    run_card_file='cards/run_card_background.dat',\n",
    "    pythia8_card_file='cards/pythia8_card.dat',\n",
    "    log_directory='logs/background',\n",
    "    initial_command='source activate python2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, two options might be very useful for larger projects:\n",
    "- `MadMiner.run_multiple()` allows you to start multiple runs with different run cards or different choices of `sample_benchmark`.\n",
    "- Both `MadMiner.run()` and `MadMiner.run_multiple()` have a `only_create_script` keyword. If that is set to True, MadMiner will not start the event generation directly, but prepare folders with all the right settings and ready-to-run bash scripts. This might make it much easier to generate Events on a high-performance computing system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run detector simulation and extract observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `madminer.delphes` wraps around Delphes, a popular fast detector simulation. In addition to simulating the detector, it allows for the fast extraction of observables, which are saved in the MadMiner HDF5 file. The central object is an instance of the `DelphesProcessor` class, which has to be initialized with a MadMiner file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:50  \n",
      "17:50  ------------------------------------------------------------\n",
      "17:50  |                                                          |\n",
      "17:50  |  MadMiner v2018.10.26                                    |\n",
      "17:50  |                                                          |\n",
      "17:50  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "17:50  |                                                          |\n",
      "17:50  ------------------------------------------------------------\n",
      "17:50  \n"
     ]
    }
   ],
   "source": [
    "dp = DelphesProcessor('data/madminer_example.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the DelphesProcessor object, one can add a number of HepMC event samples (the output of running MadGraph and Pythia in step 3) and have it run Delphes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:50  Adding HepMC sample at mg_processes/signal/Events/run_01/tag_1_pythia8_events.hepmc.gz\n"
     ]
    }
   ],
   "source": [
    "dp.add_hepmc_sample(\n",
    "    'mg_processes/signal/Events/run_01/tag_1_pythia8_events.hepmc.gz',\n",
    "    sampled_from_benchmark='sm'\n",
    ")\n",
    "dp.add_hepmc_sample(\n",
    "    'mg_processes/background/Events/run_01/tag_1_pythia8_events.hepmc.gz',\n",
    "    sampled_from_benchmark='sm'\n",
    ")\n",
    "\n",
    "dp.run_delphes(\n",
    "    delphes_directory=mg_dir + '/Delphes',\n",
    "    delphes_card='cards/delphes_card.dat',\n",
    "    log_directory='logs',\n",
    "    initial_command='source activate python2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the definition of observables through a name and a python expression. For the latter, you can use the objects `j[i]`, `e[i]`, `mu[i]`, `a[i]`, `met`, where the indices `i` refer to a ordering by the transverse momentum. All of these objects inherit from scikit-hep [LorentzVectors](http://scikit-hep.org/api/math.html#vector-classes), see the link for a documentation of their properties. In addition, they have `charge` and `pdg_id` properties.\n",
    "\n",
    "There is an optional keyword `required`. If `required=True`, we will only keep events where the observable can be parsed, i.e. all involved particles have been detected. If `required=False`, un-parseable observables will be filled with the value of another keyword `default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.add_observable('pt_j1', 'j[0].pt', required=False, default=0.)\n",
    "dp.add_observable('delta_phi_jj', '(j[0].phi() - j[1].phi()) * (-1. + 2.*float(j[0].eta() > j[1].phi()))', required=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add cuts, again in parse-able strings. In addition to the objects discussed above, they can contain the observables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.add_cut('pt_j1 > 30.')\n",
    "dp.add_cut('(j[0] + j[1]).m > 250.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `analyse_delphes_samples` then calculates all observables from the Delphes ROOT file(s) generated before and applies the cuts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.analyse_delphes_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the observables and the weights are then saved in the HDF5 file. It is possible to overwrite the same file, or to leave the original file intact and save all the data into a new file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.save('data/madminer_example_with_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to check some (normalized) distributions at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "for label in ['sm', 'w']:\n",
    "    plt.hist(\n",
    "        dp.observations['pt_j1'],\n",
    "        range=(0.,800.),\n",
    "        bins=20,\n",
    "        histtype='step',\n",
    "        weights=dp.weights[label],\n",
    "        label=label\n",
    "    )\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One side remark: For the detector simulation and calculation of observables, different users might have very different requirements. While a phenomenologist might be content with the fast detector simulation from Delphes, an experimental analysis might require the full simulation through Geant4. We therefore intend this part to be interchangeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine and shuffle different event samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce disk usage, you can generate several small event samples with the steps given above, and combine them now. Note that (for now) it is essential that all of them are generated with the same setup, including the same benchmark points / morphing basis!\n",
    "\n",
    "In our case we only have one sample, so this is not strictly necessary, but we still include it for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_and_shuffle(\n",
    "    ['data/madminer_example_with_data.h5'],\n",
    "    'data/madminer_example_shuffled.h5'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make (unweighted) training and test samples with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is handled by the `madminer.sampling` class `SampleAugmenter`. From all the data we have in the MadMiner file now, it extracts unweighted samples including the augmented data (\"gold\") that is needed as training and evaluation data for the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SampleAugmenter('data/madminer_example_shuffled.h5', debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SampleAugmenter` class defines five different high-level functions to generate train or test samples:\n",
    "- `extract_samples_train_plain()`, which only saves observations x, for instance for histograms or ABC;\n",
    "- `extract_samples_train_local()` for methods like SALLY and SALLINO, which will be demonstrated in the second part of the tutorial;\n",
    "- `extract_samples_train_ratio()` for techniques like CARL, ROLR, CASCAL, and RASCAL, when only theta0 is parameterized;\n",
    "- `extract_samples_train_more_ratios()` for the same techniques, but with both theta0 and theta1 parameterized;\n",
    "- `extract_samples_test()` for the evaluation of any method.\n",
    "\n",
    "For the arguments `theta`, `theta0`, or `theta1`, you can use the helper functions `constant_benchmark_theta()`, `multiple_benchmark_thetas()`, `constant_morphing_theta()`, `multiple_morphing_thetas()`, and `random_morphing_thetas()`, all defined in the `madminer.sampling` module.\n",
    "\n",
    "Here we'll train a likelihood ratio estimator with the RASCAL method, so we focus on the `extract_samples_train_ratio()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, theta0, theta1, y, r_xz, t_xz = sa.extract_samples_train_ratio(\n",
    "    theta0=random_morphing_thetas(100, [('gaussian', 0., 4.), ('gaussian', 0., 4.)]),\n",
    "    theta1=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='train1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation we'll need a test sample, and we'll make two just for fun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, theta = sa.extract_samples_test(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='test'\n",
    ")\n",
    "\n",
    "x_bsm, theta_bsm = sa.extract_samples_test(\n",
    "    theta=constant_benchmark_theta('w'),\n",
    "    n_samples=100000,\n",
    "    folder='./data/samples',\n",
    "    filename='test_bsm'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some distributions and correlations between these two samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "labels = [r'$p_{T,j1}$ [GeV]', r'$\\Delta \\phi_{jj}$']\n",
    "ranges = [(0., 600.), (0.,6.2)]\n",
    "bins=(25,25)\n",
    "\n",
    "fig = corner.corner(x_bsm, color='C1', labels=labels, range=ranges, bins=bins)\n",
    "_ = corner.corner(x, color='C0', labels=labels, range=ranges, bins=bins, fig=fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate total cross sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas_benchmarks, xsecs_benchmarks, xsec_errors_benchmarks = sa.extract_cross_sections(\n",
    "    theta=multiple_benchmark_thetas(['sm', 'w', 'morphing_basis_vector_2', 'morphing_basis_vector_3', 'morphing_basis_vector_4', 'morphing_basis_vector_5'])\n",
    ")\n",
    "\n",
    "thetas_morphing, xsecs_morphing, xsec_errors_morphing = sa.extract_cross_sections(\n",
    "    theta=random_morphing_thetas(1000, [('gaussian', 0., 4.), ('gaussian', 0., 4.)])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmin, cmax = np.mean(xsecs_morphing) - 2 * np.std(xsecs_morphing), np.mean(xsecs_morphing) + 2 * np.std(xsecs_morphing)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "sc = plt.scatter(thetas_morphing[:,0], thetas_morphing[:,1], c=xsecs_morphing,\n",
    "            s=40., cmap='viridis', vmin=cmin, vmax=cmax,\n",
    "            marker='o')\n",
    "\n",
    "plt.scatter(thetas_benchmarks[:,0], thetas_benchmarks[:,1], c=xsecs_benchmarks,\n",
    "            s=200., cmap='viridis', vmin=cmin, vmax=cmax, lw=2., edgecolor='black',\n",
    "            marker='s')\n",
    "\n",
    "cb = plt.colorbar(sc)\n",
    "cb.set_label('xsec [pb]')\n",
    "\n",
    "plt.xlim(-1.,1.)\n",
    "plt.ylim(-1.,1.)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What  you see here is a morphing algorithm in action. We only asked MadGraph to calculate event weights (differential cross sections, or basically squared matrix elements) at six fixed parameter points (shown here as squares with black edges). But with our knowledge about the structure of the process we can interpolate any observable to any parameter point without loss (except that statistical uncertainties might increase)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train neural networks to estimate likelihood ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to build the neural network that estimates the likelihood ratio. The central object for this is the `madminer.ml.MLForge` class. It defines functions that train, save, load, and evaluate the estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the RASCAL method described in [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013) and [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020). Other implemented methods include CARL, CASCAL, and ROLR described in the same publications. There is also SCANDAL introduced in [\"Mining gold from implicit models to improve likelihood-free inference\"](https://arxiv.org/abs/1805.12244) as well as ALICE and ALICES which are introduced in [\"Likelihood-free inference with an improved cross-entropy estimator\"](https://arxiv.org/abs/1808.00973).\n",
    "\n",
    "Most of these methods exist both in a \"single parameterized\" version, in which only the dependence of the likelihood ratio on the numerator is modelled, and a \"doubly parameterized\" version, in which both the dependence on the numerator and denominator parameters is modelled. For the single parameterized version, use `method='rascal'`, `method='alice'`, and so on. For the double parameterized version, use `method='rascal2'`, `method='alice2'`, etc. Note that for the doubly parameterized estimators you have to provide `theta1_filename`, and in the case of RASCAL and ALICE also `t_xz1_filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.train(\n",
    "    method='rascal',\n",
    "    theta0_filename='data/samples/theta0_train1.npy',\n",
    "    x_filename='data/samples/x_train1.npy',\n",
    "    y_filename='data/samples/y_train1.npy',\n",
    "    r_xz_filename='data/samples/r_xz_train1.npy',\n",
    "    t_xz0_filename='data/samples/t_xz_train1.npy',\n",
    "    n_hidden=(20,20),\n",
    "    alpha=10.,\n",
    "    n_epochs=20,\n",
    "    validation_split=0.3,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "forge.save('models/rascal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forge.evaluate(theta,x)` estimated the log likelihood ratio and the score for all combination between the given phase-space points `x` and parameters `theta`. That is, if given 100 events `x` and a grid of 25 `theta` points, it will return 25\\*100 estimates for the log likelihood and 25\\*100 estimates for the  score, both indexed by `[i_theta,i_x]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_each = np.linspace(-10.,10.,51)\n",
    "theta0, theta1 = np.meshgrid(theta_each, theta_each)\n",
    "theta_grid = np.vstack((theta0.flatten(), theta1.flatten())).T\n",
    "np.save('data/samples/theta_grid.npy', theta_grid)\n",
    "\n",
    "theta_denom = np.array([[0.,0.]])\n",
    "np.save('data/samples/theta_ref.npy', theta_denom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge.load('models/rascal')\n",
    "\n",
    "log_r_hat, _, _ = forge.evaluate(\n",
    "    theta0_filename='data/samples/theta_grid.npy',\n",
    "    x_filename='data/samples/x_test.npy',\n",
    "    evaluate_score=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_size = theta_each[1] - theta_each[0]\n",
    "edges = np.linspace(theta_each[0] - bin_size/2, theta_each[-1] + bin_size/2, len(theta_each)+1)\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = plt.gca()\n",
    "\n",
    "expected_llr = np.mean(log_r_hat,axis=1)\n",
    "best_fit = theta_grid[np.argmin(-2.*expected_llr)]\n",
    "\n",
    "cmin, cmax = np.min(-2*expected_llr), np.max(-2*expected_llr)\n",
    "    \n",
    "pcm = ax.pcolormesh(edges, edges, -2. * expected_llr.reshape((51,51)),\n",
    "                    norm=matplotlib.colors.Normalize(vmin=cmin, vmax=cmax),\n",
    "                    cmap='viridis_r')\n",
    "cbar = fig.colorbar(pcm, ax=ax, extend='both')\n",
    "\n",
    "plt.scatter(best_fit[0], best_fit[1], s=50., color='black', marker='*')\n",
    "plt.scatter(0., 0., s=50., color='black', marker='o')\n",
    "\n",
    "plt.xlabel(r'$\\theta_0$')\n",
    "plt.ylabel(r'$\\theta_1$')\n",
    "cbar.set_label(r'$\\mathbb{E}_x [ -2\\, \\log \\,\\hat{r}(x | \\theta, \\theta_{SM}) ]$ (RASCAL)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for now. Please have a look at the documentation for a detailed description of all classes and functions. And if you're curious about SALLY, Fisher information matrices, and ensemble methods, please look at the second part of the tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
