{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example\n",
    "\n",
    "Original notebook by Alexander Held\n",
    "\n",
    "Edited for the MadMiner repository by Johann Brehmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use a simple toy example to demonstrate the machine learning and inference algorithms in MadMiner. This allows us to skip many of the more technical steps, there's no dependency on MadGraph, Pythia, or Delphes.\n",
    "\n",
    "What this tutorial does not do, is explaining the inference methods. To understand what's happening, please have a look at [\"Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00013), which will explain the basic idea in just a few pages. If you really want to get down to the dirty details, [\"A Guide to Constraining Effective Field Theories With Machine Learning\"](https://arxiv.org/abs/1805.00020) has you covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import corner\n",
    "\n",
    "from madminer.ml import MLForge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MadMiner uses the Python `logging` module to provide additional information and debugging output. You can choose how much of this output you want to see by switching the level in the following lines to `logging.DEBUG` or `logging.WARNING`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MadMiner output\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)-5.5s %(name)-20.20s %(levelname)-7.7s %(message)s',\n",
    "    datefmt='%H:%M',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Output of all other modules (e.g. matplotlib)\n",
    "for key in logging.Logger.manager.loggerDict:\n",
    "    if \"madminer\" not in key:\n",
    "        logging.getLogger(key).setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simulator depends on just one parameter `theta` and produces one-dimensional observables `x`. It is characterized by one latent variable `z`. \"Running\" the simulator consists of two steps:\n",
    "- \"Hard process\": a value of `z` is drawn from a normal distribution, where the mean depends on `theta`. \n",
    "- \"Detector\": a value for `x` is drawn from a normal distribution with mean equal to `z`. There is no explicit dependence on `theta`.\n",
    "\n",
    "As in the particle physics case, we assume that we can calculate the joint likelihood ratio `r(x, z | theta0, theta1)` as well as the joint score `t(x, z | theta0, theta1)`, which depend explicitly on `z` (and where the \"detector\" part cancels).\n",
    "\n",
    "Here are some general settings (feel free to play with them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_mean(theta):\n",
    "    return theta + 10.\n",
    "\n",
    "z_std = 2.\n",
    "x_std = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function runs the simulator for a value of `theta` and calculates the joint likelihood ratio between `theta0` and `theta1` as well as the joint score at `theta_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(theta, theta0=None, theta1=None, theta_score=None, npoints=None):\n",
    "    # Draw latent variable\n",
    "    z = np.random.normal(loc=z_mean(theta), scale=z_std, size=npoints)\n",
    "    \n",
    "    # Draw observable\n",
    "    x = np.random.normal(loc=z, scale=x_std, size=None)\n",
    "    \n",
    "    # Calculate joint likelihood ratio and joint score\n",
    "    if theta0 is not None and theta1 is not None:\n",
    "        r_xz = norm(loc=z_mean(theta0), scale=z_std).pdf(z) / norm(loc=z_mean(theta1), scale=z_std).pdf(z)\n",
    "    else:\n",
    "        r_xz = None\n",
    "        \n",
    "    if theta_score is not None:\n",
    "        t_xz = (x - z_mean(theta_score)) / z_std**2\n",
    "    else:\n",
    "        t_xz = None\n",
    "    \n",
    "    return x, r_xz, t_xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the likelihood function `p(x|theta)` is intractable, but in this toy example it is just given by the convolution of two Gaussians, which is again a Gaussian. We will use this to validate the results later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood_ratio(x, theta0, theta1=0.):\n",
    "    combined_std = (z_std**2 + x_std**2)**0.5\n",
    "    r_x = norm(loc=z_mean(theta0), scale=combined_std).pdf(x) / norm(loc=z_mean(theta1), scale=combined_std).pdf(x)\n",
    "    return r_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run the simulation and generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the parameter points we want to use for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_param_points = 100000 # number of parameter points to train\n",
    "\n",
    "theta0 = np.random.uniform(low=-4.0, high=4.0, size=n_param_points) # numerator, uniform prior\n",
    "theta1 = np.zeros(shape=n_param_points)                             # denominator: fixed at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the simulator (one sample per parameter point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from theta0\n",
    "x_from_theta0, r_xz_from_theta0, t_xz_from_theta0 = simulate(theta0, theta0, theta1, theta0)\n",
    "\n",
    "# Sample from theta1\n",
    "x_from_theta1, r_xz_from_theta1, t_xz_from_theta0 = simulate(theta1, theta0, theta1, theta0)\n",
    "\n",
    "# Combine results and reshape\n",
    "x_train = np.hstack((x_from_theta0, x_from_theta1)).reshape(-1,1)\n",
    "r_xz_train = np.hstack((x_from_theta0, x_from_theta1)).reshape(-1,1)\n",
    "t_xz_train = np.hstack((x_from_theta0, x_from_theta1)).reshape(-1,1)\n",
    "y_train = np.hstack((np.zeros_like(x_from_theta0), np.ones_like(np.ones_like(x_from_theta1)))).reshape(-1,1)\n",
    "theta0_train = np.hstack((theta0, theta0)).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save everything to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/theta0_train.npy', theta0_train)\n",
    "np.save('data/x_train.npy', x_train)\n",
    "np.save('data/y_train.npy', y_train)\n",
    "np.save('data/r_xz_train.npy', r_xz_train)\n",
    "np.save('data/t_xz_train.npy', t_xz_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a single neural network to estimate the likelihood ratio, using the ALICES method introduced in [\"Likelihood-free inference with an improved cross-entropy estimator\"](https://arxiv.org/abs/1808.00973)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:38 madminer.ml          INFO    Starting training\n",
      "15:38 madminer.ml          INFO      Method:                 alices\n",
      "15:38 madminer.ml          INFO      Training data: x at data/x_train.npy\n",
      "15:38 madminer.ml          INFO                     theta0 at data/theta0_train.npy\n",
      "15:38 madminer.ml          INFO                     y at data/y_train.npy\n",
      "15:38 madminer.ml          INFO                     r_xz at data/r_xz_train.npy\n",
      "15:38 madminer.ml          INFO                     t_xz (theta0) at  data/t_xz_train.npy\n",
      "15:38 madminer.ml          INFO      Features:               all\n",
      "15:38 madminer.ml          INFO      Method:                 alices\n",
      "15:38 madminer.ml          INFO      Hidden layers:          (20, 20)\n",
      "15:38 madminer.ml          INFO      Activation function:    tanh\n",
      "15:38 madminer.ml          INFO      Batch size:             128\n",
      "15:38 madminer.ml          INFO      Trainer:                amsgrad\n",
      "15:38 madminer.ml          INFO      Epochs:                 20\n",
      "15:38 madminer.ml          INFO      Learning rate:          0.001 initially, decaying to 0.0001\n",
      "15:38 madminer.ml          INFO      Validation split:       0.3\n",
      "15:38 madminer.ml          INFO      Early stopping:         True\n",
      "15:38 madminer.ml          INFO      Scale inputs:           True\n",
      "15:38 madminer.ml          INFO      Shuffle labels          False\n",
      "15:38 madminer.ml          INFO      Regularization:         None\n",
      "15:38 madminer.ml          INFO      Samples:                all\n",
      "15:38 madminer.ml          INFO    Loading training data\n",
      "15:38 madminer.ml          INFO    Found 200000 samples with 1 parameters and 1 observables\n",
      "15:38 madminer.ml          INFO    Rescaling inputs\n",
      "15:38 madminer.ml          INFO    Creating model for method alices\n",
      "15:38 madminer.ml          INFO    Training model\n",
      "15:39 madminer.utils.ml.ra INFO      Epoch 02: train loss 2.9435 (improved_xe: 1.1909, mse_score: 17.5258)\n",
      "15:39 madminer.utils.ml.ra INFO                val. loss  2.6841 (improved_xe: 1.2648, mse_score: 14.1218) (*)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a2de0d25cc07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/ml.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, method, x_filename, y_filename, theta0_filename, theta1_filename, r_xz_filename, t_xz0_filename, t_xz1_filename, features, nde_type, n_hidden, activation, maf_n_mades, maf_batch_norm, maf_batch_norm_alpha, maf_mog_n_components, alpha, trainer, n_epochs, batch_size, initial_lr, final_lr, nesterov_momentum, validation_split, early_stopping, scale_inputs, shuffle_labels, grad_x_regularization, limit_samplesize, return_first_loss)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"some\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0mgrad_x_regularization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_x_regularization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mreturn_first_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_first_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             )\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/projects/madminer/madminer/madminer/utils/ml/ratio_trainer.py\u001b[0m in \u001b[0;36mtrain_ratio_model\u001b[0;34m(model, loss_functions, method_type, theta0s, theta1s, xs, ys, r_xzs, t_xz0s, t_xz1s, loss_weights, loss_labels, calculate_model_score, batch_size, trainer, initial_learning_rate, final_learning_rate, nesterov_momentum, n_epochs, clip_gradient, run_on_gpu, double_precision, validation_split, early_stopping, early_stopping_patience, grad_x_regularization, learning_curve_folder, learning_curve_filename, return_first_loss, verbose)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# Calculate gradient and update optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;31m# Clip gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/higgs_inference/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "forge = MLForge()\n",
    "\n",
    "forge.train(\n",
    "    method='alices',\n",
    "    x_filename='data/x_train.npy',\n",
    "    y_filename='data/y_train.npy',\n",
    "    theta0_filename='data/theta0_train.npy',\n",
    "    r_xz_filename='data/r_xz_train.npy',\n",
    "    t_xz0_filename='data/t_xz_train.npy',\n",
    "    alpha=0.1,\n",
    "    n_epochs=20,\n",
    "    n_hidden=(20,20),\n",
    "    validation_split=0.3,\n",
    "    batch_size=128,\n",
    ")\n",
    "\n",
    "forge.save('models/alices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate evaluation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate one test set from theta = 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_param_points_test = 10000 # number of parameter points to test\n",
    "\n",
    "theta_test = 1. * np.ones(shape=n_param_points_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulator to get observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, _, _ = simulate(theta_test)\n",
    "\n",
    "np.save('data/x_test.npy', x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate the expected likelihood ratio on a range of parameter points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_grid = np.linspace(-5.,5.,100).reshape(-1, 1)\n",
    "\n",
    "np.save('data/theta_grid.npy', theta_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this toy example, we can calculate the true likelihood ratio. We will save the expected log likelihood ratio (multiplied with a conventional factor of -2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nllr_test_true = []\n",
    "\n",
    "for theta in theta_grid:\n",
    "    r = calculate_likelihood_ratio(x_test, theta)\n",
    "    nllr_test_true.append(-2. * np.mean(np.log(r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to evaluate our likelihood ratio estimator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forge = MLForge(debug=False)\n",
    "forge.load('models/model_toy')\n",
    "\n",
    "log_r, _, _ = forge.evaluate(\n",
    "    theta0_filename='data/theta_grid.npy',\n",
    "    x='data/x_test.npy',\n",
    "    evaluate_score=False\n",
    ")\n",
    "\n",
    "nllr_test_alices = -2. * np.mean(log_r, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the expected log likelihood ratio over parameter space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit = float(theta_grid[np.argmin(nllr_test_alices)])\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.plot(theta_grid, nllr_test_true, label='Ground truth')\n",
    "plt.plot(theta_grid, nllr_test_alices, label='ALICES')\n",
    "\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel(r'$\\mathbb{E}_x [ -2\\, \\log \\,r(x | \\theta, \\theta_{1}) ]$')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Best fit point: theta = {}\".format(best_fit))\n",
    "print(\"True:           theta = {}\".format(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (higgs_inference)",
   "language": "python",
   "name": "higgs_inference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
